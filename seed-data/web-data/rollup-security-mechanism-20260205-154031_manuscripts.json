{
  "manuscript_v2": "# Rollup Security Mechanisms: A Comprehensive Analysis of Trust Assumptions, Verification Paradigms, and Emerging Threat Vectors\n\n## Executive Summary\n\nRollups have emerged as the dominant scaling paradigm for blockchain networks, processing over $50 billion in total value locked (TVL) across major implementations as of late 2024. These Layer 2 (L2) solutions execute transactions off-chain while inheriting security guarantees from underlying Layer 1 (L1) networks through cryptographic and economic mechanisms. This report provides a comprehensive examination of rollup security architectures, analyzing the fundamental trust assumptions, verification mechanisms, and attack surfaces that define the security posture of these systems.\n\nOur analysis reveals that rollup security is not monolithic but rather a composite of multiple interdependent mechanisms including state commitment schemes, fraud and validity proofs, data availability guarantees, sequencer designs, and bridge architectures. We examine the two primary rollup paradigms\u2014optimistic rollups and zero-knowledge (ZK) rollups\u2014identifying their respective security tradeoffs, maturity levels, and vulnerability profiles.\n\nKey findings indicate that while rollups significantly improve scalability, they introduce novel trust assumptions often overlooked in simplified security models. Current implementations frequently rely on training wheels\u2014centralized components such as upgradeable contracts, permissioned sequencers, and security councils\u2014that deviate from the trustless ideal. We identify critical attack vectors including sequencer manipulation, data withholding attacks, bridge exploits, and proof system vulnerabilities, providing quantitative analysis of historical incidents and their root causes.\n\nThe report concludes with an assessment of emerging security trends, including shared sequencer networks, based rollups, proof aggregation, and formal verification advances. We argue that rollup security will increasingly depend on defense-in-depth strategies combining cryptographic guarantees with economic incentives and social consensus mechanisms.\n\n---\n\n## 1. Introduction\n\n### 1.1 The Scaling Imperative and Rollup Emergence\n\nBlockchain scalability has remained a persistent challenge since Bitcoin's inception. Ethereum's mainnet processes approximately 15-30 transactions per second (TPS), fundamentally constraining its utility for global-scale applications. Various scaling approaches have been proposed, including sharding, state channels, plasma, and sidechains, each presenting distinct security tradeoffs.\n\nRollups emerged from this landscape as a particularly compelling solution, first conceptualized in detail by Barry Whitehat in 2018 and subsequently formalized through implementations like Optimism, Arbitrum, zkSync, and StarkNet. The rollup paradigm's core insight is the separation of execution from consensus: transactions are executed off-chain by specialized operators while transaction data and state commitments are posted to the L1, enabling independent verification.\n\nThis architecture achieves scalability through compression and batching\u2014a single L1 transaction can represent thousands of L2 transactions\u2014while theoretically preserving L1 security guarantees. However, the precise nature of these security guarantees and the conditions under which they hold require careful examination.\n\n### 1.2 Scope and Methodology\n\nThis report examines rollup security mechanisms through multiple analytical lenses:\n\n1. **Cryptographic security**: Proof systems, commitment schemes, and cryptographic assumptions\n2. **Economic security**: Incentive structures, stake requirements, and game-theoretic properties\n3. **Operational security**: Sequencer designs, upgrade mechanisms, and governance structures\n4. **Systemic security**: Cross-layer interactions, composability risks, and failure modes\n\nOur analysis draws on protocol specifications, academic literature, audit reports, and empirical data from production deployments. We adopt a threat modeling approach, systematically identifying adversarial capabilities and evaluating countermeasures.\n\n---\n\n## 2. Foundational Security Architecture\n\n### 2.1 State Commitment and Verification\n\nThe fundamental security primitive in rollups is the state commitment\u2014a cryptographic representation of the L2 state posted to L1. This commitment enables verification that L2 state transitions follow protocol rules without requiring L1 nodes to re-execute all L2 transactions.\n\n**Merkle Tree Commitments**\n\nMost rollups employ Merkle tree structures to commit to state. The state root, a 32-byte hash, represents the entire L2 state including account balances, contract storage, and nonces. Arbitrum and Optimism use variants of Ethereum's Modified Merkle Patricia Trie, while zkSync Era employs a sparse Merkle tree optimized for ZK circuit efficiency.\n\nThe security of Merkle commitments relies on collision resistance of the underlying hash function. For SHA-256 or Keccak-256, finding collisions requires approximately 2^128 operations, providing 128-bit security against collision attacks. However, ZK-rollups often use algebraically structured hash functions like Poseidon or Rescue, which enable efficient circuit representation but have received less cryptanalytic scrutiny. Security analyses of Poseidon suggest adequate security margins against known algebraic attacks (including Gr\u00f6bner basis attacks), though parameters are typically set conservatively\u2014for instance, using wider state widths than strictly necessary\u2014to account for uncertainty in the cryptanalytic landscape.\n\n**State Transition Verification**\n\nThe verification mechanism differentiates rollup paradigms:\n\n- **Optimistic rollups** assume state transitions are valid unless challenged. A challenge period (typically 7 days) allows observers to submit fraud proofs demonstrating invalid transitions.\n\n- **ZK-rollups** require validity proofs\u2014cryptographic proofs that state transitions are correct\u2014before L1 acceptance. No challenge period is necessary as validity is mathematically guaranteed (subject to cryptographic assumptions).\n\n### 2.2 Data Availability Requirements\n\nData availability (DA) is arguably the most critical security property for rollups. Users must be able to reconstruct the L2 state from publicly available data to verify correctness and exit to L1 if necessary.\n\n**The Data Availability Problem**\n\nConsider an adversarial sequencer that posts a valid state root but withholds the underlying transaction data. Without this data:\n\n- Users cannot verify the state transition's correctness\n- Users cannot construct fraud proofs (optimistic rollups)\n- Users cannot generate inclusion proofs for withdrawals\n- The rollup state becomes effectively frozen\n\n**DA Solutions Spectrum**\n\nCurrent implementations employ various DA strategies:\n\n| DA Layer | Examples | Security Model | Cost (per byte) |\n|----------|----------|----------------|-----------------|\n| Ethereum calldata | Arbitrum One, Optimism | Ethereum consensus | ~16 gas |\n| Ethereum blobs (EIP-4844) | Most major rollups | Ethereum consensus | ~1 gas equivalent |\n| Dedicated DA layers | Celestia, EigenDA, Avail | Independent consensus | Variable |\n| Validiums | Immutable X, some StarkEx | Committee/DAC | Minimal |\n\nThe security implications are significant. Ethereum-native DA inherits full Ethereum security, requiring an attacker to control Ethereum consensus to withhold data. External DA layers introduce additional trust assumptions\u2014users must trust the DA layer's consensus mechanism and honest majority assumptions.\n\n**External DA Layer Security Analysis**\n\nEach external DA solution presents distinct security properties:\n\n- **Celestia**: Uses a Tendermint-based BFT consensus requiring 2/3 honest stake. Security depends on the economic value staked and the assumption that validators cannot be simultaneously compromised or colluding. Liveness failures in Celestia would prevent rollups from posting new data, potentially halting L2 progress.\n\n- **EigenDA**: Leverages restaked ETH through EigenLayer, inheriting partial Ethereum economic security. However, the slashing conditions and attribution mechanisms differ from native Ethereum security, and correlated slashing risks across multiple AVSs (Actively Validated Services) could affect security guarantees.\n\n- **Data Availability Committees (DACs)**: Used by validiums, these typically require m-of-n committee members to attest to data availability. Security degrades to trusting that at least m members are honest and available\u2014a significantly weaker assumption than blockchain consensus.\n\n**EIP-4844 and Danksharding**\n\nEthereum's Dencun upgrade (March 2024) introduced blob transactions, providing dedicated DA space for rollups. Blobs offer approximately 10x cost reduction compared to calldata while maintaining Ethereum's security guarantees. Each blob contains 128 KB of data, with a target of 3 blobs per block (384 KB) and maximum of 6 blobs (768 KB).\n\nThe security model for blobs differs subtly from calldata. Blob data is guaranteed available for approximately 18 days (4096 epochs) but is not permanently stored by consensus nodes. Rollups must ensure archival solutions exist for historical data reconstruction.\n\n**Data Availability Sampling (DAS)**\n\nThe full vision of Danksharding relies on Data Availability Sampling, a mechanism that enables DA verification without requiring nodes to download all data. Understanding DAS is critical for evaluating future DA scaling:\n\n1. **Mechanism**: Data is encoded using erasure coding (typically Reed-Solomon), expanding it such that any 50% of the encoded data suffices to reconstruct the original. Nodes randomly sample small portions of the encoded data and verify KZG polynomial commitments.\n\n2. **Security Model**: DAS requires an *honest minority* assumption\u2014if a sufficient number of light clients perform random sampling, the probability that unavailable data goes undetected becomes negligible. Specifically, with n samples of k chunks each, the probability of missing unavailability is approximately (0.5)^(n\u00d7k).\n\n3. **Current Status**: EIP-4844 does not implement full DAS; it provides blob space with full download requirements. Full Danksharding with DAS remains on Ethereum's roadmap but requires additional infrastructure (peer-to-peer sampling protocols, KZG ceremony completion).\n\n4. **Security Implications**: DAS transforms DA from a binary property (all nodes download everything) to a probabilistic guarantee. This enables scaling but introduces new attack vectors\u2014an adversary might attempt to make data selectively unavailable to specific samplers.\n\n### 2.3 Finality Inheritance and L1-L2 Security Relationships\n\nA precise understanding of how L2 security relates to L1 finality is essential for evaluating rollup security guarantees.\n\n**Finality Hierarchy**\n\nRollup transactions pass through multiple finality stages with distinct security properties:\n\n| Finality Stage | Time | Security Guarantee | Trust Assumption |\n|----------------|------|-------------------|------------------|\n| Sequencer confirmation | ~1-2 seconds | Soft confirmation | Trust sequencer honesty |\n| L1 inclusion | ~12 seconds | L1 consensus security | L1 not reorged |\n| L1 finality | ~12-15 minutes | Cryptoeconomic finality | >1/3 ETH not slashed |\n| Challenge period completion (optimistic) | ~7 days | Full rollup security | \u22651 honest verifier |\n| Validity proof verification (ZK) | Minutes-hours | Cryptographic security | Proof system soundness |\n\n**L1 Reorg Handling**\n\nL2 state can be affected by L1 reorgs before L1 finality:\n\n1. **Deposit Reorgs**: If an L1 deposit transaction is reorged out, the corresponding L2 mint becomes invalid. Rollups typically wait for sufficient L1 confirmations (e.g., 12-64 blocks) before crediting deposits.\n\n2. **Batch Reorgs**: If a posted L2 batch is reorged, the sequencer must resubmit. Well-designed rollups handle this gracefully, but applications building on L2 should understand that pre-finality state is provisional.\n\n3. **Withdrawal Implications**: Withdrawals initiated during a reorged period may need reprocessing. The challenge period (optimistic) or proof generation (ZK) restarts from the new canonical L1 state.\n\n**Ethereum's Gasper Finality**\n\nEthereum's consensus (Gasper = Casper FFG + LMD-GHOST) provides finality after two epochs (~12.8 minutes) under normal conditions. The finality guarantee is cryptoeconomic: reverting a finalized block requires at least 1/3 of staked ETH to be slashed (~$15B at current stake levels). This finality guarantee propagates to L2:\n\n- L2 state derived from finalized L1 blocks inherits this cryptoeconomic security\n- L2 state derived from non-finalized L1 blocks remains provisional\n- Applications requiring strong guarantees should wait for L1 finality before considering L2 state settled\n\n---\n\n## 3. Optimistic Rollup Security Mechanisms\n\n### 3.1 Fraud Proof Systems\n\nOptimistic rollups derive their name from the optimistic assumption that posted state roots are valid. Security relies on the ability to prove and penalize invalid state transitions through fraud proofs.\n\n**Interactive Fraud Proofs (Arbitrum)**\n\nArbitrum employs a multi-round interactive dispute resolution protocol:\n\n1. **Assertion**: Validator posts state commitment (RBlock) with stake (currently ~3,600 ETH \u2248 $10M)\n2. **Challenge**: Observer disputes by staking and identifying disagreement point\n3. **Bisection**: Parties iteratively narrow disagreement to single instruction through O(log n) rounds\n4. **One-step proof**: L1 contract executes single WAVM instruction to determine correctness\n5. **Resolution**: Losing party's stake is slashed; winner receives portion as reward\n\nThis design minimizes on-chain computation\u2014only one instruction is ever executed on L1\u2014while maintaining security. The protocol is secure under the assumption that at least one honest validator monitors the chain and can submit challenges within the dispute window.\n\n```\nDispute Resolution Complexity:\n- Rounds required: O(log n) where n = number of instructions in disputed block\n- For n = 2^40 instructions: ~40 bisection rounds\n- On-chain verification: O(1) - single instruction execution\n- Total time: ~7 days (challenge period) + bisection rounds (~hours to days)\n```\n\n**Non-Interactive Fraud Proofs (Optimism Cannon)**\n\nOptimism's Cannon fault proof system (launched 2024) generates non-interactive proofs:\n\n1. **Assertion**: Proposer posts output root with bond (currently 0.08 ETH)\n2. **Challenge**: Challenger posts competing claim with bond\n3. **Bisection game**: On-chain bisection to identify first divergent state\n4. **Execution**: MIPS VM executes disputed instruction on-chain\n\nThe key innovation is the use of a minimal MIPS instruction set, enabling deterministic re-execution of any disputed computation. This approach reduces trust assumptions compared to earlier implementations that relied on a Security Council for dispute resolution.\n\n**Fraud Proof Re-execution Requirements**\n\nGenerating a fraud proof requires the challenger to:\n\n1. **Obtain full transaction data**: All inputs to the disputed state transition must be available\n2. **Re-execute the state transition**: Compute the correct post-state locally\n3. **Identify the divergence point**: Determine where the asserted state differs from correct state\n4. **Generate the proof**: Construct Merkle proofs and witness data for on-chain verification\n\nThis process requires significant computational resources and access to historical state data, creating practical barriers to fraud proof submission beyond the theoretical 1-of-n honest verifier assumption.\n\n### 3.2 Challenge Period Security Analysis\n\nThe 7-day challenge period is a critical security parameter with significant implications requiring rigorous analysis.\n\n**Honest Verifier Assumption**\n\nSecurity requires at least one honest, well-resourced verifier to:\n- Monitor all state assertions continuously\n- Detect invalid transitions through independent re-execution\n- Submit timely challenges with sufficient stake\n- Participate in dispute resolution through completion\n\nThis is weaker than the honest majority assumption required by L1 consensus but still represents a meaningful trust assumption. If all verifiers are compromised, collude, or are censored, invalid state transitions could be finalized.\n\n**Quantitative Challenge Period Analysis**\n\nThe 7-day period must be sufficient for:\n\n1. **Detection time**: Verifiers must notice invalid assertions. With continuous monitoring, this is near-instant; without it, detection depends on monitoring frequency.\n\n2. **Proof generation time**: Constructing fraud proofs requires re-executing the disputed computation and generating Merkle proofs. For complex state transitions, this may require hours.\n\n3. **L1 inclusion time**: The fraud proof transaction must be included in an L1 block. Under normal conditions, this takes minutes; under congestion, potentially hours.\n\n4. **Dispute resolution time**: Interactive protocols (Arbitrum) require multiple rounds of on-chain interaction, potentially taking days.\n\n**Adversarial Conditions Analysis**\n\nUnder adversarial conditions, the 7-day window may be stressed:\n\n*Gas Price Attack Scenario*:\n- Attacker submits invalid state root\n- Simultaneously floods L1 mempool to spike gas prices\n- If gas prices exceed verifier budgets, fraud proofs may be delayed\n\n*Quantitative Analysis*:\n```\nAssumptions:\n- Fraud proof gas cost: ~1-5M gas\n- Sustained attack gas price: 500 gwei\n- Fraud proof cost at 500 gwei: 0.5-2.5 ETH per proof\n- Verifier budget: Variable\n\nAttack sustainability:\n- Ethereum processes ~15M gas/block, ~7200 blocks/day\n- Sustaining 500 gwei for 7 days requires mass transaction spam\n- Estimated attack cost: >$100M in gas fees alone\n- Detection: Highly visible, would trigger social intervention\n```\n\nThis analysis suggests that pure gas price attacks are economically impractical for sustained periods, though short-term censorship remains possible.\n\n*L1 Censorship Scenario*:\n- If >50% of L1 proposers collude to censor fraud proofs\n- Censorship must be sustained for full 7 days\n- Highly detectable through missed slots and inclusion delays\n- Would likely trigger social consensus intervention (hard fork consideration)\n\n**Minimum Verifier Capital Requirements**\n\nFor a verifier to guarantee fraud proof submission:\n\n```\nMinimum Capital = Stake Requirement + Max Gas Costs + Operational Buffer\n\nArbitrum:\n- Stake: 3,600 ETH (~$10M)\n- Gas (worst case 7-day congestion): ~10 ETH\n- Operational costs: Variable\n- Total: ~$10M minimum\n\nOptimism:\n- Bond: 0.08 ETH per challenge\n- Gas costs: Similar to Arbitrum\n- Lower barrier but also lower economic security\n```\n\n**Challenge Period Adequacy Conclusion**\n\nThe 7-day period appears adequate under realistic adversarial models given:\n- Economic cost of sustained censorship/congestion attacks\n- High visibility of such attacks enabling social response\n- Multiple independent verifiers reducing single-point-of-failure risk\n\nHowever, the period assumes verifiers have sufficient capital and operational capability. Proposals for dynamic challenge periods (extending under detected adversarial conditions) could provide additional security margins.\n\n### 3.3 Sequencer Security\n\nThe sequencer is responsible for ordering transactions, executing them, and posting batches to L1. Sequencer security encompasses multiple dimensions:\n\n**Centralized Sequencer Risks**\n\nCurrent major optimistic rollups (Arbitrum One, OP Mainnet) operate single, permissioned sequencers controlled by their respective foundations. This introduces several risks:\n\n1. **Liveness failures**: Sequencer downtime halts L2 transaction processing\n2. **Censorship**: Sequencer can selectively exclude transactions\n3. **MEV extraction**: Sequencer has monopoly on transaction ordering\n4. **Front-running**: Sequencer can observe and front-run pending transactions\n\n**Forced Inclusion Mechanism Analysis**\n\nForced inclusion provides censorship resistance by allowing users to submit transactions directly to L1:\n\n*Arbitrum Delayed Inbox*:\n- Users submit transactions to L1 `SequencerInbox` contract\n- Sequencer must include within 24 hours or users can force inclusion\n- Force inclusion requires L1 transaction (~100K gas)\n\n*Security Analysis*:\n```\nScenario: Sequencer censors user, user attempts forced inclusion\n\nTimeline:\nT+0: User submits to delayed inbox\nT+24h: Force inclusion available if not included\nT+24h+: User calls forceInclusion()\n\nAdversarial Case: Sequencer AND L1 proposers collude\n- L1 proposers could censor forceInclusion() calls\n- Requires >50% L1 proposer collusion\n- Same security assumption as L1 censorship resistance\n```\n\nThe forced inclusion mechanism effectively inherits L1's censorship resistance properties with a 24-hour delay. Under the assumption that L1 remains censorship-resistant, users can always eventually transact.\n\n**Sequencer-L1 Proposer Collusion**\n\nA sophisticated attack involves collusion between the sequencer and L1 block proposers:\n\n1. Sequencer submits invalid state root\n2. Colluding L1 proposers censor fraud proofs\n3. If sustained for 7 days, invalid state finalizes\n\n*Mitigation Analysis*:\n- Requires controlling >50% of L1 proposing power for 7 days\n- Current Ethereum has ~900K validators; controlling majority requires ~$15B+ stake\n- Attack is detectable through inclusion delay metrics\n- Social layer would likely intervene before 7 days\n\nThis attack vector, while theoretically possible, requires resources and coordination that make it impractical against well-monitored rollups.\n\n**Decentralized Sequencer Roadmaps**\n\nBoth Arbitrum and Optimism have announced plans for sequencer decentralization:\n\n- **Arbitrum Timeboost**: Auction mechanism for transaction ordering rights within time windows, distributing MEV while maintaining performance\n- **Optimism**: Sequencer rotation within Superchain ecosystem, with plans for permissionless sequencing\n\nThese approaches aim to distribute MEV and reduce single points of failure while maintaining performance characteristics.\n\n---\n\n## 4. Zero-Knowledge Rollup Security Mechanisms\n\n### 4.1 Validity Proof Systems\n\nZK-rollups replace the challenge period with cryptographic validity proofs, fundamentally altering the security model.\n\n**Proof System Taxonomy**\n\n| System | Proof Size | Verification Cost | Prover Complexity | Trust Setup |\n|--------|-----------|-------------------|-------------------|-------------|\n| Groth16 | ~200 bytes | ~200K gas (~3 pairings) | O(n log n) FFTs + O(n) MSMs | Trusted (per-circuit) |\n| PLONK | ~400 bytes | ~300K gas (~2 pairings + O(1) field ops) | O(n log n) FFTs + O(n) MSMs | Universal (updatable) |\n| STARKs | ~50-200 KB | ~1-2M gas (hash-based) | O(n log\u00b2 n) hash operations | Transparent |\n| Halo2 | ~5-10 KB | ~500K gas | O(n log n) + recursion overhead | Transparent |\n\n**Formal Security Definitions**\n\nValidity proofs must satisfy precise security properties:\n\n1. **Completeness**: For any valid statement x with witness w, an honest prover can produce a proof \u03c0 that verifies.\n   ```\n   Pr[Verify(vk, x, Prove(pk, x, w)) = 1] = 1\n   ```\n\n2. **Computational Soundness**: No probabilistic polynomial-time (PPT) adversary can produce a valid proof for a false statement except with negligible probability.\n   ```\n   Pr[Verify(vk, x, \u03c0) = 1 \u2227 x \u2209 L] \u2264 negl(\u03bb)\n   ```\n\n3. **Knowledge Soundness**: For ZK-rollups, we require the stronger property that any prover producing a valid proof must \"know\" a valid witness. Formally, there exists an extractor E such that:\n   ```\n   Pr[Verify(vk, x, \u03c0) = 1 \u2227 (x, w) \u2209 R] \u2264 negl(\u03bb)\n   ```\n   where w = E(prover's internal state).\n\nKnowledge soundness is critical for rollups because it ensures that a valid proof corresponds to an actual valid state transition, not merely that invalid proofs are hard to forge.\n\n4. **Zero-Knowledge**: The proof reveals nothing about the witness beyond the statement's truth. For rollups, this property is often relaxed since transaction data is typically public anyway.\n\n**Soundness Error and Security Parameters**\n\nThe soundness error\u2014probability of a false proof verifying\u2014depends on the proof system and parameters:\n\n- **Groth16**: Soundness error \u2248 1/|F| where F is the field. For BN254 (common choice), |F| \u2248 2^254, giving ~254-bit soundness.\n- **PLONK**: Similar field-based soundness with additional security from polynomial commitment scheme.\n- **STARKs**: Soundness error depends on FRI parameters. With blowup factor \u03c1 and q queries: error \u2248 max(1/|F|, (1/\u03c1)^q). Typical parameters achieve 128-bit security.\n\n### 4.2 Cryptographic Assumptions Analysis\n\nDifferent proof systems rely on different hardness assumptions with varying levels of confidence:\n\n**Assumption Hierarchy (Roughly Ordered by Strength)**\n\n```\nWeaker (More Confidence)\n    \u2193\nCollision-Resistant Hash Functions (CRHFs)\n    \u2193\nDiscrete Logarithm Problem (DLP)\n    \u2193\nComputational Diffie-Hellman (CDH)\n    \u2193\nDecisional Diffie-Hellman (DDH)\n    \u2193\nKnowledge of Exponent (KEA)\n    \u2193\nq-Strong Diffie-Hellman (q-SDH)\n    \u2193\nAlgebraic Group Model (AGM)\n    \u2193\nStronger (Less Confidence)\n```\n\n**Proof System Assumptions**\n\n- **Groth16**: Requires q-SDH and Knowledge of Exponent Assumption (KEA) in bilinear groups. KEA is non-falsifiable\u2014we cannot construct an efficient test to check if it holds\u2014making it a stronger assumption. Additionally, Groth16 requires a per-circuit trusted setup; compromise of the setup toxic waste allows proof forgery.\n\n- **PLONK**: Requires DDH and AGM assumptions. The universal (updatable) setup is an improvement over Groth16\u2014a single ceremony works for all circuits up to a size bound, and the setup can be updated to add new randomness, reducing trust in any single participant.\n\n- **STARKs**: Rely on collision-resistant hash functions and the Random Oracle Model (ROM) for the Fiat-Shamir transformation. The ROM is an idealization\u2014real hash functions are not random oracles\u2014but this is a well-studied assumption with strong empirical support. Importantly, STARKs avoid algebraic assumptions vulnerable to quantum computers.\n\n- **Halo2**: Uses the discrete logarithm assumption in elliptic curves without pairings, plus techniques from the IPA (Inner Product Argument) for polynomial commitments. No trusted setup required.\n\n**Trusted Setup Implications**\n\nFor Groth16-based systems, trusted setup compromise is catastrophic:\n\n- The setup generates \"toxic waste\" (trapdoor values) that must be destroyed\n- If any participant retains the toxic waste, they can forge proofs for any statement\n- Multi-party computation (MPC) ceremonies distribute trust: if at least one participant is honest and destroys their contribution, the setup is secure\n- Practical implication: Users must trust that at least one ceremony participant was honest\n\nPLONK's universal setup reduces this risk:\n- Single ceremony covers all circuits up to a size\n- Setup can be updated by anyone to add new randomness\n- Each update makes previous toxic waste insufficient for forgery\n\nSTARKs eliminate setup trust entirely\u2014security relies only on public hash functions.\n\n**Post-Quantum Considerations**\n\nQuantum computers threaten elliptic curve cryptography:\n\n- **Vulnerable**: Groth16, PLONK, Halo2 (Shor's algorithm breaks DLP)\n- **Resistant**: STARKs (hash-based, no algebraic structure)\n\nMigration paths for SNARK-based rollups:\n1. Transition to STARK-based proofs (StarkNet's approach)\n2. Adopt post-quantum SNARKs based on lattice assumptions (active research)\n3. Hybrid systems using both classical and post-quantum proofs\n\nTimeline considerations: Cryptographically relevant quantum computers are estimated 10-20+ years away, but migration should begin well in advance given the complexity of these systems.\n\n### 4.3 Circuit Security and Formal Verification\n\nZK-rollups require encoding the state transition function as an arithmetic circuit. Circuit bugs represent a critical attack vector.\n\n**Historical Vulnerabilities**\n\nSeveral significant circuit vulnerabilities have been discovered:\n\n1. **zkSync Era (2023)**: Vulnerability in ECRECOVER precompile implementation could allow signature forgery. The bug was in the circuit constraints for elliptic curve operations\u2014certain edge cases were not properly constrained, allowing invalid signatures to produce valid proofs. Patched before exploitation; $50K bug bounty paid.\n\n2. **Polygon zkEVM (2024)**: Soundness bug in Keccak circuit could allow proof of invalid state transitions. The constraint system did not fully enforce the Keccak permutation, allowing adversarial inputs to satisfy constraints without performing correct hashing. Found by security researchers; patched promptly.\n\n3. **PSE (Privacy & Scaling Explorations) Circuits**: Multiple issues found in community circuit libraries, highlighting the difficulty of correct constraint implementation.\n\n**Root Cause Analysis**\n\nCommon circuit vulnerability patterns:\n\n1. **Under-constrained witnesses**: Circuits allow witness values that don't correspond to valid computations\n2. **Missing range checks**: Failure to constrain field elements to expected ranges\n3. **Incorrect field arithmetic**: Edge cases in modular arithmetic not handled\n4. **Precompile implementation errors**: Complex operations (ECRECOVER, Keccak, etc.) incorrectly constrained\n\n**Formal Verification Approaches**\n\nLeading ZK-rollups invest heavily in formal verification:\n\n- **StarkNet/Cairo**: Cairo language designed with formal verification in mind. The Cairo VM has a relatively simple semantics amenable to formal analysis. StarkWare has worked on formal proofs of Cairo program correctness.\n\n- **zkSync/Boojum**: Extensive use of formal methods for circuit verification. Internal tooling for automated constraint checking and symbolic execution of circuits.\n\n- **Polygon zkEVM**: Collaboration with academic institutions (notably, formal verification researchers) on proving circuit correctness. Published formal specifications of EVM semantics for verification.\n\n**Verification Challenges**\n\nComplete formal verification of EVM-equivalent circuits faces significant challenges:\n\n```\nCircuit Complexity (approximate):\n- Simple transfer: ~10K constraints\n- ERC-20 transfer: ~100K constraints  \n- Complex DeFi transaction: ~1M constraints\n- Full EVM block: ~10M-100M constraints\n\nVerification State Space:\n- Each constraint involves field elements (254-bit values)\n- Interactions between constraints create combinatorial complexity\n- Full formal verification of all edge cases remains intractable\n```\n\n**Defense in Depth Strategy**\n\nGiven verification challenges, ZK-rollups employ multiple security layers:\n\n1. **Formal verification of critical components**: Focus on highest-risk circuits (signature verification, state root computation)\n2. **Extensive testing**: Fuzzing, property-based testing, differential testing against reference implementations\n3. **Multiple independent audits**: Different auditors may catch different bug classes\n4. **Bug bounties**: Incentivize external security research ($1M+ bounties for critical vulnerabilities)\n5. **Gradual TVL increase**: Limit exposure during early deployment phases\n6. **Upgrade capability**: Ability to patch vulnerabilities quickly (with associated centralization tradeoffs)\n\n### 4.4 Prover Infrastructure Security\n\nThe prover\u2014the entity generating validity proofs\u2014represents another security consideration.\n\n**Centralized Provers**\n\nCurrent ZK-rollups operate centralized proving infrastructure:\n\n- **StarkNet**: StarkWare operates provers\n- **zkSync Era**: Matter Labs operates provers\n- **Polygon zkEVM**: Polygon Labs operates provers\n\n**Security Implications of Centralized Provers**\n\nCentralized provers create liveness dependencies but not safety risks under the assumption of sound proof systems:\n\n- **Safety**: An adversary controlling the prover cannot generate valid proofs for invalid state transitions (soundness guarantee)\n- **Liveness**: Prover failure halts new state updates; users can still withdraw using last valid state\n- **Censorship**: Prover could refuse to include certain transactions; mitigated by forced inclusion mechanisms\n\n**Prover Decentralization Challenges**\n\nDecentralizing proving is technically challenging:\n\n1. **Hardware requirements**: \n   - STARK proving: 128-512 GB RAM, high-core-count CPUs\n   - SNARK proving: Similar RAM, benefits from GPU acceleration\n   - Cost: $10K-100K per proving node\n\n2. **Proof generation time constraints**:\n   - Must meet block time requirements (seconds to minutes)\n   - Parallelization limited by proof structure\n   - Network latency for proof distribution\n\n3. **Economic sustainability**:\n   - Proving costs: $0.01-0.10 per transaction (varies widely)\n   - Must be covered by transaction fees\n   - Competition drives margins down\n\n**Emerging Solutions**\n\n- **Proof Markets**: Platforms like =nil; Foundation's Proof Market enable competitive proving, potentially reducing costs and increasing decentralization\n- **Hardware Acceleration**: FPGA and ASIC development for proving (e.g., Cysic, Ingonyama) could reduce costs 10-100x\n- **Recursive Proofs**: Aggregate multiple proofs, amortizing verification costs\n\n### 4.5 Proof Recursion and Aggregation Security\n\nRecursive proof composition\u2014proving the validity of other proofs\u2014enables powerful optimizations but introduces additional security considerations.",
  "manuscript_final_v3": "# Rollup Security Mechanisms: A Comprehensive Analysis of Trust Assumptions, Verification Paradigms, and Emerging Threat Vectors\n\n## Executive Summary\n\nRollups have emerged as the dominant scaling paradigm for blockchain networks, processing over $50 billion in total value locked (TVL) across major implementations as of late 2024. These Layer 2 (L2) solutions execute transactions off-chain while inheriting security guarantees from underlying Layer 1 (L1) networks through cryptographic and economic mechanisms. This report provides a comprehensive examination of rollup security architectures, analyzing the fundamental trust assumptions, verification mechanisms, and attack surfaces that define the security posture of these systems.\n\nOur analysis reveals that rollup security is not monolithic but rather a composite of multiple interdependent mechanisms including state commitment schemes, fraud and validity proofs, data availability guarantees, sequencer designs, bridge architectures, and upgrade governance. We examine the two primary rollup paradigms\u2014optimistic rollups and zero-knowledge (ZK) rollups\u2014identifying their respective security tradeoffs, maturity levels, and vulnerability profiles.\n\nKey findings indicate that while rollups significantly improve scalability, they introduce novel trust assumptions often overlooked in simplified security models. Current implementations frequently rely on training wheels\u2014centralized components such as upgradeable contracts, permissioned sequencers, and security councils\u2014that deviate from the trustless ideal. We identify critical attack vectors including sequencer manipulation, data withholding attacks, bridge exploits, proof system vulnerabilities, and upgrade governance risks, providing quantitative analysis of historical incidents and their root causes.\n\nThe report concludes with an assessment of emerging security trends, including shared sequencer networks, based rollups, proof aggregation, and formal verification advances. We argue that rollup security will increasingly depend on defense-in-depth strategies combining cryptographic guarantees with economic incentives and social consensus mechanisms.\n\n---\n\n## 1. Introduction\n\n### 1.1 The Scaling Imperative and Rollup Emergence\n\nBlockchain scalability has remained a persistent challenge since Bitcoin's inception. Ethereum's mainnet processes approximately 15-30 transactions per second (TPS), fundamentally constraining its utility for global-scale applications. Various scaling approaches have been proposed, including sharding, state channels, plasma, and sidechains, each presenting distinct security tradeoffs.\n\nRollups emerged from this landscape as a particularly compelling solution, first conceptualized in detail by Barry Whitehat in 2018 and subsequently formalized through implementations like Optimism, Arbitrum, zkSync, and StarkNet. The rollup paradigm's core insight is the separation of execution from consensus: transactions are executed off-chain by specialized operators while transaction data and state commitments are posted to the L1, enabling independent verification.\n\nThis architecture achieves scalability through compression and batching\u2014a single L1 transaction can represent thousands of L2 transactions\u2014while theoretically preserving L1 security guarantees. However, the precise nature of these security guarantees and the conditions under which they hold require careful examination.\n\n### 1.2 Scope and Methodology\n\nThis report examines rollup security mechanisms through multiple analytical lenses:\n\n1. **Cryptographic security**: Proof systems, commitment schemes, and cryptographic assumptions\n2. **Economic security**: Incentive structures, stake requirements, and game-theoretic properties\n3. **Operational security**: Sequencer designs, upgrade mechanisms, and governance structures\n4. **Smart contract security**: Bridge contracts, withdrawal mechanisms, and upgrade paths\n5. **Systemic security**: Cross-layer interactions, composability risks, and failure modes\n\nOur analysis draws on protocol specifications, academic literature, audit reports, and empirical data from production deployments. We adopt a threat modeling approach, systematically identifying adversarial capabilities and evaluating countermeasures.\n\n---\n\n## 2. Foundational Security Architecture\n\n### 2.1 State Commitment and Verification\n\nThe fundamental security primitive in rollups is the state commitment\u2014a cryptographic representation of the L2 state posted to L1. This commitment enables verification that L2 state transitions follow protocol rules without requiring L1 nodes to re-execute all L2 transactions.\n\n**Merkle Tree Commitments**\n\nMost rollups employ Merkle tree structures to commit to state. The state root, a 32-byte hash, represents the entire L2 state including account balances, contract storage, and nonces. Arbitrum and Optimism use variants of Ethereum's Modified Merkle Patricia Trie, while zkSync Era employs a sparse Merkle tree optimized for ZK circuit efficiency.\n\nThe security of Merkle commitments relies on collision resistance of the underlying hash function. For SHA-256 or Keccak-256, finding collisions requires approximately 2^128 operations, providing 128-bit security against collision attacks. However, ZK-rollups often use algebraically structured hash functions like Poseidon or Rescue, which enable efficient circuit representation but have received less cryptanalytic scrutiny. Security analyses of Poseidon suggest adequate security margins against known algebraic attacks (including Gr\u00f6bner basis attacks), though parameters are typically set conservatively\u2014for instance, using wider state widths than strictly necessary\u2014to account for uncertainty in the cryptanalytic landscape.\n\n**State Transition Verification**\n\nThe verification mechanism differentiates rollup paradigms:\n\n- **Optimistic rollups** assume state transitions are valid unless challenged. A challenge period (typically 7 days) allows observers to submit fraud proofs demonstrating invalid transitions.\n\n- **ZK-rollups** require validity proofs\u2014cryptographic proofs that state transitions are correct\u2014before L1 acceptance. No challenge period is necessary as validity is mathematically guaranteed (subject to cryptographic assumptions).\n\n### 2.2 Data Availability Requirements\n\nData availability (DA) is arguably the most critical security property for rollups. Users must be able to reconstruct the L2 state from publicly available data to verify correctness and exit to L1 if necessary.\n\n**The Data Availability Problem**\n\nConsider an adversarial sequencer that posts a valid state root but withholds the underlying transaction data. Without this data:\n\n- Users cannot verify the state transition's correctness\n- Users cannot construct fraud proofs (optimistic rollups)\n- Users cannot generate inclusion proofs for withdrawals\n- The rollup state becomes effectively frozen\n\n**DA Solutions Spectrum**\n\nCurrent implementations employ various DA strategies:\n\n| DA Layer | Examples | Security Model | Cost (per byte) |\n|----------|----------|----------------|-----------------|\n| Ethereum calldata | Arbitrum One, Optimism | Ethereum consensus | ~16 gas |\n| Ethereum blobs (EIP-4844) | Most major rollups | Ethereum consensus | ~1 gas equivalent |\n| Dedicated DA layers | Celestia, EigenDA, Avail | Independent consensus | Variable |\n| Validiums | Immutable X, some StarkEx | Committee/DAC | Minimal |\n\nThe security implications are significant. Ethereum-native DA inherits full Ethereum security, requiring an attacker to control Ethereum consensus to withhold data. External DA layers introduce additional trust assumptions\u2014users must trust the DA layer's consensus mechanism and honest majority assumptions.\n\n**External DA Layer Security Analysis**\n\nEach external DA solution presents distinct security properties:\n\n- **Celestia**: Uses a Tendermint-based BFT consensus requiring 2/3 honest stake. Security depends on the economic value staked and the assumption that validators cannot be simultaneously compromised or colluding. Liveness failures in Celestia would prevent rollups from posting new data, potentially halting L2 progress.\n\n- **EigenDA**: Leverages restaked ETH through EigenLayer, inheriting partial Ethereum economic security. However, the slashing conditions and attribution mechanisms differ from native Ethereum security, and correlated slashing risks across multiple AVSs (Actively Validated Services) could affect security guarantees.\n\n- **Data Availability Committees (DACs)**: Used by validiums, these typically require m-of-n committee members to attest to data availability. Security degrades to trusting that at least m members are honest and available\u2014a significantly weaker assumption than blockchain consensus.\n\n**Security Composition with External DA**\n\nWhen rollups use external DA layers, the security analysis becomes more complex than simply taking the minimum of L1 and DA layer security. The composition involves:\n\n1. **Execution correctness**: Depends on the rollup's proof system (fraud proofs or validity proofs)\n2. **Data availability**: Depends on the DA layer's consensus and honest majority assumptions\n3. **Ordering and inclusion**: Depends on both the sequencer and L1 inclusion\n4. **Finality**: Requires both L1 finality AND DA layer finality for full settlement\n\nFor a Celestia-based rollup, the security reduction is not straightforward. Celestia's 2/3 honest stake assumption differs from Ethereum's\u2014Celestia has different validator sets, economic security levels, and slashing conditions. A formal security argument would require showing that breaking the rollup's security requires breaking *either* Ethereum consensus *or* Celestia consensus (for availability) *and* the rollup's proof system (for correctness). The interaction between these assumptions under adversarial conditions (e.g., coordinated attacks on both layers) remains an area requiring further formal analysis.\n\n**EIP-4844 and Danksharding**\n\nEthereum's Dencun upgrade (March 2024) introduced blob transactions, providing dedicated DA space for rollups. Blobs offer approximately 10x cost reduction compared to calldata while maintaining Ethereum's security guarantees. Each blob contains 128 KB of data, with a target of 3 blobs per block (384 KB) and maximum of 6 blobs (768 KB).\n\nThe security model for blobs differs subtly from calldata. Blob data is guaranteed available for approximately 18 days (4096 epochs) but is not permanently stored by consensus nodes. This creates a critical dependency on archival infrastructure that affects the \"inherit L1 security\" claim.\n\n**Blob Data Expiration and Archival Security**\n\nThe 18-day blob pruning window introduces trust assumptions not present with permanent calldata storage:\n\n1. **Archival Node Requirements**: After blob expiration, rollup state reconstruction depends on archival nodes that voluntarily store historical blob data. Users must trust that:\n   - Sufficient archival nodes exist and remain operational\n   - Archival nodes store data correctly and completely\n   - Archival data remains accessible when needed\n\n2. **Security Implications**:\n   - *Fraud proof generation*: For optimistic rollups, fraud proofs for old state transitions require historical transaction data. If this data is only available from archival nodes, the 1-of-n honest verifier assumption extends to requiring at least one honest *and data-complete* verifier.\n   - *Historical withdrawal proofs*: Users who haven't withdrawn before blob expiration must rely on archival data to construct withdrawal proofs.\n   - *State reconstruction*: New nodes joining the network cannot independently verify the full history using only L1 data after blob expiration.\n\n3. **Mitigation Strategies**:\n   - Rollups maintaining their own archival infrastructure (centralization tradeoff)\n   - Decentralized storage networks (Filecoin, Arweave) with economic incentives for persistence\n   - Portal Network and similar protocols for distributed historical data access\n   - Requiring users to maintain withdrawal proofs locally before blob expiration\n\nThe practical implication is that rollup security with blob DA is time-bounded: full L1-equivalent security guarantees apply only within the 18-day window. Beyond this window, security degrades to trust in archival infrastructure.\n\n**Data Availability Sampling (DAS)**\n\nThe full vision of Danksharding relies on Data Availability Sampling, a mechanism that enables DA verification without requiring nodes to download all data. Understanding DAS is critical for evaluating future DA scaling:\n\n1. **Mechanism**: Data is encoded using erasure coding (typically Reed-Solomon), expanding it such that any 50% of the encoded data suffices to reconstruct the original. Nodes randomly sample small portions of the encoded data and verify KZG polynomial commitments.\n\n2. **Security Model**: DAS requires an *honest minority* assumption\u2014if a sufficient number of light clients perform random sampling, the probability that unavailable data goes undetected becomes negligible. Specifically, with n samples of k chunks each, the probability of missing unavailability is approximately (0.5)^(n\u00d7k).\n\n3. **Network-Level Requirements**: For the honest minority assumption to hold in practice, the peer-to-peer protocol must ensure:\n   - Random sampling is truly random and not predictable by adversaries\n   - Samplers cannot be eclipsed or selectively served different data\n   - Sufficient independent samplers participate in each sampling round\n   - Network latency does not allow adversaries to selectively reveal data\n\n4. **Current Status**: EIP-4844 does not implement full DAS; it provides blob space with full download requirements. Full Danksharding with DAS remains on Ethereum's roadmap but requires additional infrastructure (peer-to-peer sampling protocols, KZG ceremony completion).\n\n5. **Security Implications**: DAS transforms DA from a binary property (all nodes download everything) to a probabilistic guarantee. This enables scaling but introduces new attack vectors\u2014an adversary might attempt to make data selectively unavailable to specific samplers or exploit network-level vulnerabilities to bias sampling.\n\n**The Fisherman's Dilemma in DAS**\n\nDAS security relies on sufficient nodes performing random sampling, but this creates a public goods problem:\n\n- Individual samplers bear computational and bandwidth costs\n- Security benefits accrue to all users regardless of individual sampling\n- Rational actors may free-ride on others' sampling efforts\n- If too many free-ride, sampling density becomes insufficient for security\n\nProposed mitigations include:\n- Protocol-level incentives for sampling (rewards for samplers)\n- Requiring sampling as part of light client operation\n- Social/reputational incentives within validator communities\n- Minimum sampling requirements enforced by client software defaults\n\n### 2.3 Finality Inheritance and L1-L2 Security Relationships\n\nA precise understanding of how L2 security relates to L1 finality is essential for evaluating rollup security guarantees.\n\n**Finality Hierarchy**\n\nRollup transactions pass through multiple finality stages with distinct security properties:\n\n| Finality Stage | Time | Security Guarantee | Trust Assumption |\n|----------------|------|-------------------|------------------|\n| Sequencer confirmation | ~1-2 seconds | Soft confirmation | Trust sequencer honesty |\n| L1 inclusion | ~12 seconds | L1 consensus security | L1 not reorged |\n| L1 finality | ~12-15 minutes | Cryptoeconomic finality | >1/3 ETH not slashed |\n| Challenge period completion (optimistic) | ~7 days | Full rollup security | \u22651 honest verifier |\n| Validity proof verification (ZK) | Minutes-hours | Cryptographic security | Proof system soundness |\n\n**Clarifying \"Inherited Security\"**\n\nThe claim that L2 \"inherits L1 security\" requires careful qualification. Different security properties are inherited differently:\n\n1. **Data availability**: Fully inherited when using Ethereum DA (calldata or blobs within pruning window). Users can independently verify data was posted by checking L1.\n\n2. **Ordering finality**: Inherited after L1 finality. The order of L2 transactions is determined by their L1 inclusion order, which becomes immutable after Ethereum finalization.\n\n3. **Execution correctness**: \n   - *Optimistic rollups*: Inherited probabilistically, contingent on the 1-of-n honest verifier assumption and challenge period completion\n   - *ZK rollups*: Inherited cryptographically, contingent on proof system soundness\n\n4. **Censorship resistance**: Partially inherited through forced inclusion mechanisms, but with delays (typically 24 hours) and requiring L1 transaction inclusion\n\n5. **Value security**: Depends on bridge contract correctness, upgrade governance, and all of the above properties\n\n**L1 Reorg Handling**\n\nL2 state can be affected by L1 reorgs before L1 finality:\n\n1. **Deposit Reorgs**: If an L1 deposit transaction is reorged out, the corresponding L2 mint becomes invalid. Rollups typically wait for sufficient L1 confirmations (e.g., 12-64 blocks) before crediting deposits.\n\n2. **Batch Reorgs**: If a posted L2 batch is reorged, the sequencer must resubmit. Well-designed rollups handle this gracefully, but applications building on L2 should understand that pre-finality state is provisional.\n\n3. **Withdrawal Implications**: Withdrawals initiated during a reorged period may need reprocessing. The challenge period (optimistic) or proof generation (ZK) restarts from the new canonical L1 state.\n\n**Ethereum's Gasper Finality**\n\nEthereum's consensus (Gasper = Casper FFG + LMD-GHOST) provides finality after two epochs (~12.8 minutes) under normal conditions. The finality guarantee is cryptoeconomic: reverting a finalized block requires at least 1/3 of staked ETH to be slashed (~$15B at current stake levels). This finality guarantee propagates to L2:\n\n- L2 state derived from finalized L1 blocks inherits this cryptoeconomic security\n- L2 state derived from non-finalized L1 blocks remains provisional\n- Applications requiring strong guarantees should wait for L1 finality before considering L2 state settled\n\n---\n\n## 3. Optimistic Rollup Security Mechanisms\n\n### 3.1 Fraud Proof Systems\n\nOptimistic rollups derive their name from the optimistic assumption that posted state roots are valid. Security relies on the ability to prove and penalize invalid state transitions through fraud proofs.\n\n**Interactive Fraud Proofs (Arbitrum)**\n\nArbitrum employs a multi-round interactive dispute resolution protocol:\n\n1. **Assertion**: Validator posts state commitment (RBlock) with stake (currently ~3,600 ETH \u2248 $10M)\n2. **Challenge**: Observer disputes by staking and identifying disagreement point\n3. **Bisection**: Parties iteratively narrow disagreement to single instruction through O(log n) rounds\n4. **One-step proof**: L1 contract executes single WAVM instruction to determine correctness\n5. **Resolution**: Losing party's stake is slashed; winner receives portion as reward\n\nThis design minimizes on-chain computation\u2014only one instruction is ever executed on L1\u2014while maintaining security. The protocol is secure under the assumption that at least one honest validator monitors the chain and can submit challenges within the dispute window.\n\n```\nDispute Resolution Complexity:\n- Rounds required: O(log n) where n = number of instructions in disputed block\n- For n = 2^40 instructions: ~40 bisection rounds\n- On-chain verification: O(1) - single instruction execution\n- Total time: ~7 days (challenge period) + bisection rounds (~hours to days)\n```\n\n**Non-Interactive Fraud Proofs (Optimism Cannon)**\n\nOptimism's Cannon fault proof system (launched 2024) generates non-interactive proofs:\n\n1. **Assertion**: Proposer posts output root with bond (currently 0.08 ETH)\n2. **Challenge**: Challenger posts competing claim with bond\n3. **Bisection game**: On-chain bisection to identify first divergent state\n4. **Execution**: MIPS VM executes disputed instruction on-chain\n\nThe key innovation is the use of a minimal MIPS instruction set, enabling deterministic re-execution of any disputed computation. This approach reduces trust assumptions compared to earlier implementations that relied on a Security Council for dispute resolution.\n\n**Fraud Proof Re-execution Requirements**\n\nGenerating a fraud proof requires the challenger to:\n\n1. **Obtain full transaction data**: All inputs to the disputed state transition must be available\n2. **Re-execute the state transition**: Compute the correct post-state locally\n3. **Identify the divergence point**: Determine where the asserted state differs from correct state\n4. **Generate the proof**: Construct Merkle proofs and witness data for on-chain verification\n\nThis process requires significant computational resources and access to historical state data, creating practical barriers to fraud proof submission beyond the theoretical 1-of-n honest verifier assumption.\n\n### 3.2 Challenge Period Security Analysis\n\nThe 7-day challenge period is a critical security parameter with significant implications requiring rigorous analysis.\n\n**Honest Verifier Assumption**\n\nSecurity requires at least one honest, well-resourced verifier to:\n- Monitor all state assertions continuously\n- Detect invalid transitions through independent re-execution\n- Submit timely challenges with sufficient stake\n- Participate in dispute resolution through completion\n\nThis is weaker than the honest majority assumption required by L1 consensus but still represents a meaningful trust assumption. If all verifiers are compromised, collude, or are censored, invalid state transitions could be finalized.\n\n**Quantitative Challenge Period Analysis**\n\nThe 7-day period must be sufficient for:\n\n1. **Detection time**: Verifiers must notice invalid assertions. With continuous monitoring, this is near-instant; without it, detection depends on monitoring frequency.\n\n2. **Proof generation time**: Constructing fraud proofs requires re-executing the disputed computation and generating Merkle proofs. For complex state transitions, this may require hours.\n\n3. **L1 inclusion time**: The fraud proof transaction must be included in an L1 block. Under normal conditions, this takes minutes; under congestion, potentially hours.\n\n4. **Dispute resolution time**: Interactive protocols (Arbitrum) require multiple rounds of on-chain interaction, potentially taking days.\n\n**Adversarial Conditions Analysis**\n\nUnder adversarial conditions, the 7-day window may be stressed:\n\n*Gas Price Attack Scenario*:\n- Attacker submits invalid state root\n- Simultaneously floods L1 mempool to spike gas prices\n- If gas prices exceed verifier budgets, fraud proofs may be delayed\n\n*Quantitative Analysis*:\n```\nAssumptions:\n- Fraud proof gas cost: ~1-5M gas\n- Sustained attack gas price: 500 gwei\n- Fraud proof cost at 500 gwei: 0.5-2.5 ETH per proof\n- Verifier budget: Variable\n\nAttack sustainability:\n- Ethereum processes ~15M gas/block, ~7200 blocks/day\n- Sustaining 500 gwei for 7 days requires mass transaction spam\n- Estimated attack cost: >$100M in gas fees alone\n- Detection: Highly visible, would trigger social intervention\n```\n\nThis analysis suggests that pure gas price attacks are economically impractical for sustained periods, though short-term censorship remains possible.\n\n*L1 Censorship Scenario*:\n- If >50% of L1 proposers collude to censor fraud proofs\n- Censorship must be sustained for full 7 days\n- Highly detectable through missed slots and inclusion delays\n- Would likely trigger social consensus intervention (hard fork consideration)\n\n**Builder Centralization and PBS Dynamics**\n\nThe validator-count analysis above understates censorship risk due to Proposer-Builder Separation (PBS) dynamics. Currently:\n\n- ~3 builders produce >90% of Ethereum blocks through MEV-boost\n- Builders, not validators, determine transaction inclusion in most blocks\n- A colluding set of dominant builders could censor fraud proofs more easily than the ~900K validator count suggests\n\n*Revised Censorship Analysis*:\n```\nScenario: Top 3 builders collude to censor fraud proofs\n\nBlock production share: ~90%\nProbability of 7 days without inclusion through honest builder:\n- ~10% of blocks from non-colluding builders\n- ~7200 blocks/day \u00d7 7 days = 50,400 blocks\n- Expected honest blocks: ~5,040\n- Probability of censorship success: negligible\n\nHowever, for shorter windows or higher builder concentration:\n- If 99% builder collusion: ~504 honest blocks over 7 days\n- Still likely sufficient, but margin reduced significantly\n```\n\nThe PBS architecture creates a more concentrated censorship surface than pure validator analysis suggests. Mitigations include:\n- Inclusion lists (proposed protocol change forcing proposers to include certain transactions)\n- Builder diversity initiatives\n- Direct validator submission bypassing builders (higher cost, lower MEV extraction)\n\n**Minimum Verifier Capital Requirements**\n\nFor a verifier to guarantee fraud proof submission:\n\n```\nMinimum Capital = Stake Requirement + Max Gas Costs + Operational Buffer\n\nArbitrum:\n- Stake: 3,600 ETH (~$10M)\n- Gas (worst case 7-day congestion): ~10 ETH\n- Operational costs: Variable\n- Total: ~$10M minimum\n\nOptimism:\n- Bond: 0.08 ETH per challenge\n- Gas costs: Similar to Arbitrum\n- Lower barrier but also lower economic security\n```\n\n**Economic Sustainability Under Sustained Attack**\n\nThe forced inclusion escape hatch's economic sustainability under sustained attack conditions deserves analysis:\n\n```\nSustained Attack Scenario:\n- Attacker maintains L1 gas prices at 500 gwei for 7 days\n- User forced inclusion cost: ~100K gas \u00d7 500 gwei = 0.05 ETH per transaction\n- At $3000/ETH: ~$150 per forced transaction\n\nImplications:\n- Small-value transactions become uneconomical to force-include\n- Users with <$150 at risk may be unable to exit economically\n- Attacker cost to maintain: >$100M (as calculated above)\n- Rational attacker targets high-value situations where gains exceed attack cost\n```\n\n**Challenge Period Adequacy for Withdrawals**\n\nThe challenge period analysis must specifically address withdrawal security:\n\n1. **Withdrawal proof construction**: Users must generate Merkle proofs against the finalized state root. If the state root is fraudulent, honest verifiers must challenge before users can withdraw against the invalid state.\n\n2. **Withdrawal censorship**: An attacker could:\n   - Submit invalid state root\n   - Censor fraud proofs (as analyzed above)\n   - Censor withdrawal transactions from victims\n   - Execute withdrawals for attacker-controlled accounts\n\n3. **Withdrawal contract attack surface**: Even with valid state roots, withdrawal contract bugs could allow:\n   - Proof verification bypass\n   - Double withdrawals\n   - Unauthorized withdrawals\n\nThese withdrawal-specific concerns are addressed in detail in Section 5.\n\n**Challenge Period Adequacy Conclusion**\n\nThe 7-day period appears adequate under realistic adversarial models given:\n- Economic cost of sustained censorship/congestion attacks\n- High visibility of such attacks enabling social response\n- Multiple independent verifiers reducing single-point-of-failure risk\n\nHowever, the period assumes verifiers have sufficient capital and operational capability, and builder centralization creates a more concentrated censorship surface than validator counts suggest. Proposals for dynamic challenge periods (extending under detected adversarial conditions) could provide additional security margins.\n\n### 3.3 Sequencer Security\n\nThe sequencer is responsible for ordering transactions, executing them, and posting batches to L1. Sequencer security encompasses multiple dimensions:\n\n**Centralized Sequencer Risks**\n\nCurrent major optimistic rollups (Arbitrum One, OP Mainnet) operate single, permissioned sequencers controlled by their respective foundations. This introduces several risks:\n\n1. **Liveness failures**: Sequencer downtime halts L2 transaction processing\n2. **Censorship**: Sequencer can selectively exclude transactions\n3. **MEV extraction**: Sequencer has monopoly on transaction ordering\n4. **Front-running**: Sequencer can observe and front-run pending transactions\n\n**Forced Inclusion Mechanism Analysis**\n\nForced inclusion provides censorship resistance by allowing users to submit transactions directly to L1:\n\n*Arbitrum Delayed Inbox*:\n- Users submit transactions to L1 `SequencerInbox` contract\n- Sequencer must include within 24 hours or users can force inclusion\n- Force inclusion requires L1 transaction (~100K gas)\n\n*Security Analysis*:\n```\nScenario: Sequencer censors user, user attempts forced inclusion\n\nTimeline:\nT+0: User submits to delayed inbox\nT+24h: Force inclusion available if not included\nT+24h+: User calls forceInclusion()\n\nAdversarial Case: Sequencer AND L1 proposers collude\n- L1 proposers could censor forceInclusion() calls\n- Requires >50% L1 proposer collusion\n- Same security assumption as L1 censorship resistance\n```\n\nThe forced inclusion mechanism effectively inherits L1's censorship resistance properties with a 24-hour delay. Under the assumption that L1 remains censorship-resistant, users can always eventually transact.\n\n**24-Hour Delay Adequacy**\n\nThe 24-hour forced inclusion delay may be insufficient under extreme L1 congestion:\n\n```\nExtreme Congestion Scenario:\n- Base fee: 1000 gwei (historical peaks have exceeded this)\n- Force inclusion gas: 100K\n- Cost: 0.1 ETH (~$300)\n- Block capacity: ~1500 transactions per block\n- If demand exceeds capacity for >24 hours, forced inclusion may be delayed\n\nHistorical precedent:\n- NFT mints have caused multi-hour congestion\n- No 24-hour sustained congestion observed to date\n- But unprecedented events (major exploit, market crash) could trigger\n```\n\n**Sequencer-L1 Proposer Collusion**\n\nA sophisticated attack involves collusion between the sequencer and L1 block proposers:\n\n1. Sequencer submits invalid state root\n2. Colluding L1 proposers censor fraud proofs\n3. If sustained for 7 days, invalid state finalizes\n\n*Mitigation Analysis*:\n- Requires controlling >50% of L1 proposing power for 7 days\n- Current Ethereum has ~900K validators; controlling majority requires ~$15B+ stake\n- However, builder centralization (3 builders controlling 90% of blocks) reduces this barrier\n- Attack is detectable through inclusion delay metrics\n- Social layer would likely intervene before 7 days\n\nThis attack vector, while theoretically possible, requires resources and coordination that make it impractical against well-monitored rollups\u2014but builder centralization makes it less impractical than pure validator analysis suggests.\n\n**Decentralized Sequencer Roadmaps**\n\nBoth Arbitrum and Optimism have announced plans for sequencer decentralization:\n\n- **Arbitrum Timeboost**: Auction mechanism for transaction ordering rights within time windows, distributing MEV while maintaining performance\n- **Optimism**: Sequencer rotation within Superchain ecosystem, with plans for permissionless sequencing\n\nThese approaches aim to distribute MEV and reduce single points of failure while maintaining performance characteristics.\n\n---\n\n## 4. Zero-Knowledge Rollup Security Mechanisms\n\n### 4.1 Validity Proof Systems\n\nZK-rollups replace the challenge period with cryptographic validity proofs, fundamentally altering the security model.\n\n**Proof System Taxonomy**\n\n| System | Proof Size | Verification Cost | Prover Complexity | Trust Setup |\n|--------|-----------|-------------------|-------------------|-------------|\n| Groth16 | ~200 bytes | ~200K gas (~3 pairings) | O(n log n) FFTs + O(n) MSMs | Trusted (per-circuit) |\n| PLONK | ~400 bytes | ~300K gas (~2 pairings + O(1) field ops) | O(n log n) FFTs + O(n) MSMs | Universal (updatable) |\n| STARKs | ~50-200 KB | ~1-2M gas (hash-based) | O(n log\u00b2 n) hash operations | Transparent |\n| Halo2 | ~5-10 KB | ~500K gas | O(n log n) + recursion overhead | Transparent |\n\n**Formal Security Definitions**\n\nValidity proofs must satisfy precise security properties:\n\n1. **Completeness**: For any valid statement x with witness w, an honest prover can produce a proof \u03c0 that verifies.\n   ```\n   Pr[Verify(vk, x, Prove(pk, x, w)) = 1] = 1\n   ```\n\n2. **Computational Soundness**: No probabilistic polynomial-time (PPT) adversary can produce a valid proof for a false statement except with negligible probability.\n   ```\n   Pr[Verify(vk, x, \u03c0) = 1 \u2227 x \u2209 L] \u2264 negl(\u03bb)\n   ```\n\n3. **Knowledge Soundness**: For ZK-rollups, we require the stronger property that any prover producing a valid proof must \"know\" a valid witness. Formally, there exists an extractor E such that:\n   ```\n   Pr[Verify(vk, x, \u03c0) = 1 \u2227 (x, w) \u2209 R] \u2264 negl(\u03bb)\n   ```\n   where w = E(prover's internal state).\n\nKnowledge soundness is critical for rollups because it ensures that a valid proof corresponds to an actual valid state transition, not merely that invalid proofs are hard to forge.\n\n4. **Zero-Knowledge**: The proof reveals nothing about the witness beyond the statement's truth. For rollups, this property is often relaxed since transaction data is typically public anyway.\n\n**Soundness Error and Security Parameters**\n\nThe soundness error\u2014probability of a false proof verifying\u2014depends on the proof system and parameters:\n\n- **Groth16**: Soundness error \u2248 1/|F| where F is the field. For BN254 (common choice), |F| \u2248 2^254, giving ~254-bit soundness. The reduction to q-SDH is tight, meaning the concrete security matches the assumption's hardness.\n\n- **PLONK**: Field-based soundness with additional considerations:\n  - Polynomial commitment scheme (typically KZG) adds security dependency on discrete log in pairing groups\n  - Fiat-Shamir transformation introduces security loss in the random oracle model\n  - Algebraic Group Model (AGM) proofs provide security reductions but AGM is a strong idealization\n  - Concrete security: ~128-bit after accounting for reduction tightness and Fiat-Shamir\n\n- **STARKs**: Soundness depends on",
  "manuscript_v3": "# Rollup Security Mechanisms: A Comprehensive Analysis of Trust Assumptions, Verification Paradigms, and Emerging Threat Vectors\n\n## Executive Summary\n\nRollups have emerged as the dominant scaling paradigm for blockchain networks, processing over $50 billion in total value locked (TVL) across major implementations as of late 2024. These Layer 2 (L2) solutions execute transactions off-chain while inheriting security guarantees from underlying Layer 1 (L1) networks through cryptographic and economic mechanisms. This report provides a comprehensive examination of rollup security architectures, analyzing the fundamental trust assumptions, verification mechanisms, and attack surfaces that define the security posture of these systems.\n\nOur analysis reveals that rollup security is not monolithic but rather a composite of multiple interdependent mechanisms including state commitment schemes, fraud and validity proofs, data availability guarantees, sequencer designs, bridge architectures, and upgrade governance. We examine the two primary rollup paradigms\u2014optimistic rollups and zero-knowledge (ZK) rollups\u2014identifying their respective security tradeoffs, maturity levels, and vulnerability profiles.\n\nKey findings indicate that while rollups significantly improve scalability, they introduce novel trust assumptions often overlooked in simplified security models. Current implementations frequently rely on training wheels\u2014centralized components such as upgradeable contracts, permissioned sequencers, and security councils\u2014that deviate from the trustless ideal. We identify critical attack vectors including sequencer manipulation, data withholding attacks, bridge exploits, proof system vulnerabilities, and upgrade governance risks, providing quantitative analysis of historical incidents and their root causes.\n\nThe report concludes with an assessment of emerging security trends, including shared sequencer networks, based rollups, proof aggregation, and formal verification advances. We argue that rollup security will increasingly depend on defense-in-depth strategies combining cryptographic guarantees with economic incentives and social consensus mechanisms.\n\n---\n\n## 1. Introduction\n\n### 1.1 The Scaling Imperative and Rollup Emergence\n\nBlockchain scalability has remained a persistent challenge since Bitcoin's inception. Ethereum's mainnet processes approximately 15-30 transactions per second (TPS), fundamentally constraining its utility for global-scale applications. Various scaling approaches have been proposed, including sharding, state channels, plasma, and sidechains, each presenting distinct security tradeoffs.\n\nRollups emerged from this landscape as a particularly compelling solution, first conceptualized in detail by Barry Whitehat in 2018 and subsequently formalized through implementations like Optimism, Arbitrum, zkSync, and StarkNet. The rollup paradigm's core insight is the separation of execution from consensus: transactions are executed off-chain by specialized operators while transaction data and state commitments are posted to the L1, enabling independent verification.\n\nThis architecture achieves scalability through compression and batching\u2014a single L1 transaction can represent thousands of L2 transactions\u2014while theoretically preserving L1 security guarantees. However, the precise nature of these security guarantees and the conditions under which they hold require careful examination.\n\n### 1.2 Scope and Methodology\n\nThis report examines rollup security mechanisms through multiple analytical lenses:\n\n1. **Cryptographic security**: Proof systems, commitment schemes, and cryptographic assumptions\n2. **Economic security**: Incentive structures, stake requirements, and game-theoretic properties\n3. **Operational security**: Sequencer designs, upgrade mechanisms, and governance structures\n4. **Smart contract security**: Bridge contracts, withdrawal mechanisms, and upgrade paths\n5. **Systemic security**: Cross-layer interactions, composability risks, and failure modes\n\nOur analysis draws on protocol specifications, academic literature, audit reports, and empirical data from production deployments. We adopt a threat modeling approach, systematically identifying adversarial capabilities and evaluating countermeasures.\n\n---\n\n## 2. Foundational Security Architecture\n\n### 2.1 State Commitment and Verification\n\nThe fundamental security primitive in rollups is the state commitment\u2014a cryptographic representation of the L2 state posted to L1. This commitment enables verification that L2 state transitions follow protocol rules without requiring L1 nodes to re-execute all L2 transactions.\n\n**Merkle Tree Commitments**\n\nMost rollups employ Merkle tree structures to commit to state. The state root, a 32-byte hash, represents the entire L2 state including account balances, contract storage, and nonces. Arbitrum and Optimism use variants of Ethereum's Modified Merkle Patricia Trie, while zkSync Era employs a sparse Merkle tree optimized for ZK circuit efficiency.\n\nThe security of Merkle commitments relies on collision resistance of the underlying hash function. For SHA-256 or Keccak-256, finding collisions requires approximately 2^128 operations, providing 128-bit security against collision attacks. However, ZK-rollups often use algebraically structured hash functions like Poseidon or Rescue, which enable efficient circuit representation but have received less cryptanalytic scrutiny. Security analyses of Poseidon suggest adequate security margins against known algebraic attacks (including Gr\u00f6bner basis attacks), though parameters are typically set conservatively\u2014for instance, using wider state widths than strictly necessary\u2014to account for uncertainty in the cryptanalytic landscape.\n\n**State Transition Verification**\n\nThe verification mechanism differentiates rollup paradigms:\n\n- **Optimistic rollups** assume state transitions are valid unless challenged. A challenge period (typically 7 days) allows observers to submit fraud proofs demonstrating invalid transitions.\n\n- **ZK-rollups** require validity proofs\u2014cryptographic proofs that state transitions are correct\u2014before L1 acceptance. No challenge period is necessary as validity is mathematically guaranteed (subject to cryptographic assumptions).\n\n### 2.2 Data Availability Requirements\n\nData availability (DA) is arguably the most critical security property for rollups. Users must be able to reconstruct the L2 state from publicly available data to verify correctness and exit to L1 if necessary.\n\n**The Data Availability Problem**\n\nConsider an adversarial sequencer that posts a valid state root but withholds the underlying transaction data. Without this data:\n\n- Users cannot verify the state transition's correctness\n- Users cannot construct fraud proofs (optimistic rollups)\n- Users cannot generate inclusion proofs for withdrawals\n- The rollup state becomes effectively frozen\n\n**DA Solutions Spectrum**\n\nCurrent implementations employ various DA strategies:\n\n| DA Layer | Examples | Security Model | Cost (per byte) |\n|----------|----------|----------------|-----------------|\n| Ethereum calldata | Arbitrum One, Optimism | Ethereum consensus | ~16 gas |\n| Ethereum blobs (EIP-4844) | Most major rollups | Ethereum consensus | ~1 gas equivalent |\n| Dedicated DA layers | Celestia, EigenDA, Avail | Independent consensus | Variable |\n| Validiums | Immutable X, some StarkEx | Committee/DAC | Minimal |\n\nThe security implications are significant. Ethereum-native DA inherits full Ethereum security, requiring an attacker to control Ethereum consensus to withhold data. External DA layers introduce additional trust assumptions\u2014users must trust the DA layer's consensus mechanism and honest majority assumptions.\n\n**External DA Layer Security Analysis**\n\nEach external DA solution presents distinct security properties:\n\n- **Celestia**: Uses a Tendermint-based BFT consensus requiring 2/3 honest stake. Security depends on the economic value staked and the assumption that validators cannot be simultaneously compromised or colluding. Liveness failures in Celestia would prevent rollups from posting new data, potentially halting L2 progress.\n\n- **EigenDA**: Leverages restaked ETH through EigenLayer, inheriting partial Ethereum economic security. However, the slashing conditions and attribution mechanisms differ from native Ethereum security, and correlated slashing risks across multiple AVSs (Actively Validated Services) could affect security guarantees.\n\n- **Data Availability Committees (DACs)**: Used by validiums, these typically require m-of-n committee members to attest to data availability. Security degrades to trusting that at least m members are honest and available\u2014a significantly weaker assumption than blockchain consensus.\n\n**Security Composition with External DA**\n\nWhen rollups use external DA layers, the security analysis becomes more complex than simply taking the minimum of L1 and DA layer security. The composition involves:\n\n1. **Execution correctness**: Depends on the rollup's proof system (fraud proofs or validity proofs)\n2. **Data availability**: Depends on the DA layer's consensus and honest majority assumptions\n3. **Ordering and inclusion**: Depends on both the sequencer and L1 inclusion\n4. **Finality**: Requires both L1 finality AND DA layer finality for full settlement\n\nFor a Celestia-based rollup, the security reduction is not straightforward. Celestia's 2/3 honest stake assumption differs from Ethereum's\u2014Celestia has different validator sets, economic security levels, and slashing conditions. A formal security argument would require showing that breaking the rollup's security requires breaking *either* Ethereum consensus *or* Celestia consensus (for availability) *and* the rollup's proof system (for correctness). The interaction between these assumptions under adversarial conditions (e.g., coordinated attacks on both layers) remains an area requiring further formal analysis.\n\n**EIP-4844 and Danksharding**\n\nEthereum's Dencun upgrade (March 2024) introduced blob transactions, providing dedicated DA space for rollups. Blobs offer approximately 10x cost reduction compared to calldata while maintaining Ethereum's security guarantees. Each blob contains 128 KB of data, with a target of 3 blobs per block (384 KB) and maximum of 6 blobs (768 KB).\n\nThe security model for blobs differs subtly from calldata. Blob data is guaranteed available for approximately 18 days (4096 epochs) but is not permanently stored by consensus nodes. This creates a critical dependency on archival infrastructure that affects the \"inherit L1 security\" claim.\n\n**Blob Data Expiration and Archival Security**\n\nThe 18-day blob pruning window introduces trust assumptions not present with permanent calldata storage:\n\n1. **Archival Node Requirements**: After blob expiration, rollup state reconstruction depends on archival nodes that voluntarily store historical blob data. Users must trust that:\n   - Sufficient archival nodes exist and remain operational\n   - Archival nodes store data correctly and completely\n   - Archival data remains accessible when needed\n\n2. **Security Implications**:\n   - *Fraud proof generation*: For optimistic rollups, fraud proofs for old state transitions require historical transaction data. If this data is only available from archival nodes, the 1-of-n honest verifier assumption extends to requiring at least one honest *and data-complete* verifier.\n   - *Historical withdrawal proofs*: Users who haven't withdrawn before blob expiration must rely on archival data to construct withdrawal proofs.\n   - *State reconstruction*: New nodes joining the network cannot independently verify the full history using only L1 data after blob expiration.\n\n3. **Mitigation Strategies**:\n   - Rollups maintaining their own archival infrastructure (centralization tradeoff)\n   - Decentralized storage networks (Filecoin, Arweave) with economic incentives for persistence\n   - Portal Network and similar protocols for distributed historical data access\n   - Requiring users to maintain withdrawal proofs locally before blob expiration\n\nThe practical implication is that rollup security with blob DA is time-bounded: full L1-equivalent security guarantees apply only within the 18-day window. Beyond this window, security degrades to trust in archival infrastructure.\n\n**Data Availability Sampling (DAS)**\n\nThe full vision of Danksharding relies on Data Availability Sampling, a mechanism that enables DA verification without requiring nodes to download all data. Understanding DAS is critical for evaluating future DA scaling:\n\n1. **Mechanism**: Data is encoded using erasure coding (typically Reed-Solomon), expanding it such that any 50% of the encoded data suffices to reconstruct the original. Nodes randomly sample small portions of the encoded data and verify KZG polynomial commitments.\n\n2. **Security Model**: DAS requires an *honest minority* assumption\u2014if a sufficient number of light clients perform random sampling, the probability that unavailable data goes undetected becomes negligible. Specifically, with n samples of k chunks each, the probability of missing unavailability is approximately (0.5)^(n\u00d7k).\n\n3. **Network-Level Requirements**: For the honest minority assumption to hold in practice, the peer-to-peer protocol must ensure:\n   - Random sampling is truly random and not predictable by adversaries\n   - Samplers cannot be eclipsed or selectively served different data\n   - Sufficient independent samplers participate in each sampling round\n   - Network latency does not allow adversaries to selectively reveal data\n\n4. **Current Status**: EIP-4844 does not implement full DAS; it provides blob space with full download requirements. Full Danksharding with DAS remains on Ethereum's roadmap but requires additional infrastructure (peer-to-peer sampling protocols, KZG ceremony completion).\n\n5. **Security Implications**: DAS transforms DA from a binary property (all nodes download everything) to a probabilistic guarantee. This enables scaling but introduces new attack vectors\u2014an adversary might attempt to make data selectively unavailable to specific samplers or exploit network-level vulnerabilities to bias sampling.\n\n**The Fisherman's Dilemma in DAS**\n\nDAS security relies on sufficient nodes performing random sampling, but this creates a public goods problem:\n\n- Individual samplers bear computational and bandwidth costs\n- Security benefits accrue to all users regardless of individual sampling\n- Rational actors may free-ride on others' sampling efforts\n- If too many free-ride, sampling density becomes insufficient for security\n\nProposed mitigations include:\n- Protocol-level incentives for sampling (rewards for samplers)\n- Requiring sampling as part of light client operation\n- Social/reputational incentives within validator communities\n- Minimum sampling requirements enforced by client software defaults\n\n### 2.3 Finality Inheritance and L1-L2 Security Relationships\n\nA precise understanding of how L2 security relates to L1 finality is essential for evaluating rollup security guarantees.\n\n**Finality Hierarchy**\n\nRollup transactions pass through multiple finality stages with distinct security properties:\n\n| Finality Stage | Time | Security Guarantee | Trust Assumption |\n|----------------|------|-------------------|------------------|\n| Sequencer confirmation | ~1-2 seconds | Soft confirmation | Trust sequencer honesty |\n| L1 inclusion | ~12 seconds | L1 consensus security | L1 not reorged |\n| L1 finality | ~12-15 minutes | Cryptoeconomic finality | >1/3 ETH not slashed |\n| Challenge period completion (optimistic) | ~7 days | Full rollup security | \u22651 honest verifier |\n| Validity proof verification (ZK) | Minutes-hours | Cryptographic security | Proof system soundness |\n\n**Clarifying \"Inherited Security\"**\n\nThe claim that L2 \"inherits L1 security\" requires careful qualification. Different security properties are inherited differently:\n\n1. **Data availability**: Fully inherited when using Ethereum DA (calldata or blobs within pruning window). Users can independently verify data was posted by checking L1.\n\n2. **Ordering finality**: Inherited after L1 finality. The order of L2 transactions is determined by their L1 inclusion order, which becomes immutable after Ethereum finalization.\n\n3. **Execution correctness**: \n   - *Optimistic rollups*: Inherited probabilistically, contingent on the 1-of-n honest verifier assumption and challenge period completion\n   - *ZK rollups*: Inherited cryptographically, contingent on proof system soundness\n\n4. **Censorship resistance**: Partially inherited through forced inclusion mechanisms, but with delays (typically 24 hours) and requiring L1 transaction inclusion\n\n5. **Value security**: Depends on bridge contract correctness, upgrade governance, and all of the above properties\n\n**L1 Reorg Handling**\n\nL2 state can be affected by L1 reorgs before L1 finality:\n\n1. **Deposit Reorgs**: If an L1 deposit transaction is reorged out, the corresponding L2 mint becomes invalid. Rollups typically wait for sufficient L1 confirmations (e.g., 12-64 blocks) before crediting deposits.\n\n2. **Batch Reorgs**: If a posted L2 batch is reorged, the sequencer must resubmit. Well-designed rollups handle this gracefully, but applications building on L2 should understand that pre-finality state is provisional.\n\n3. **Withdrawal Implications**: Withdrawals initiated during a reorged period may need reprocessing. The challenge period (optimistic) or proof generation (ZK) restarts from the new canonical L1 state.\n\n**Ethereum's Gasper Finality**\n\nEthereum's consensus (Gasper = Casper FFG + LMD-GHOST) provides finality after two epochs (~12.8 minutes) under normal conditions. The finality guarantee is cryptoeconomic: reverting a finalized block requires at least 1/3 of staked ETH to be slashed (~$15B at current stake levels). This finality guarantee propagates to L2:\n\n- L2 state derived from finalized L1 blocks inherits this cryptoeconomic security\n- L2 state derived from non-finalized L1 blocks remains provisional\n- Applications requiring strong guarantees should wait for L1 finality before considering L2 state settled\n\n---\n\n## 3. Optimistic Rollup Security Mechanisms\n\n### 3.1 Fraud Proof Systems\n\nOptimistic rollups derive their name from the optimistic assumption that posted state roots are valid. Security relies on the ability to prove and penalize invalid state transitions through fraud proofs.\n\n**Interactive Fraud Proofs (Arbitrum)**\n\nArbitrum employs a multi-round interactive dispute resolution protocol:\n\n1. **Assertion**: Validator posts state commitment (RBlock) with stake (currently ~3,600 ETH \u2248 $10M)\n2. **Challenge**: Observer disputes by staking and identifying disagreement point\n3. **Bisection**: Parties iteratively narrow disagreement to single instruction through O(log n) rounds\n4. **One-step proof**: L1 contract executes single WAVM instruction to determine correctness\n5. **Resolution**: Losing party's stake is slashed; winner receives portion as reward\n\nThis design minimizes on-chain computation\u2014only one instruction is ever executed on L1\u2014while maintaining security. The protocol is secure under the assumption that at least one honest validator monitors the chain and can submit challenges within the dispute window.\n\n```\nDispute Resolution Complexity:\n- Rounds required: O(log n) where n = number of instructions in disputed block\n- For n = 2^40 instructions: ~40 bisection rounds\n- On-chain verification: O(1) - single instruction execution\n- Total time: ~7 days (challenge period) + bisection rounds (~hours to days)\n```\n\n**Non-Interactive Fraud Proofs (Optimism Cannon)**\n\nOptimism's Cannon fault proof system (launched 2024) generates non-interactive proofs:\n\n1. **Assertion**: Proposer posts output root with bond (currently 0.08 ETH)\n2. **Challenge**: Challenger posts competing claim with bond\n3. **Bisection game**: On-chain bisection to identify first divergent state\n4. **Execution**: MIPS VM executes disputed instruction on-chain\n\nThe key innovation is the use of a minimal MIPS instruction set, enabling deterministic re-execution of any disputed computation. This approach reduces trust assumptions compared to earlier implementations that relied on a Security Council for dispute resolution.\n\n**Fraud Proof Re-execution Requirements**\n\nGenerating a fraud proof requires the challenger to:\n\n1. **Obtain full transaction data**: All inputs to the disputed state transition must be available\n2. **Re-execute the state transition**: Compute the correct post-state locally\n3. **Identify the divergence point**: Determine where the asserted state differs from correct state\n4. **Generate the proof**: Construct Merkle proofs and witness data for on-chain verification\n\nThis process requires significant computational resources and access to historical state data, creating practical barriers to fraud proof submission beyond the theoretical 1-of-n honest verifier assumption.\n\n### 3.2 Challenge Period Security Analysis\n\nThe 7-day challenge period is a critical security parameter with significant implications requiring rigorous analysis.\n\n**Honest Verifier Assumption**\n\nSecurity requires at least one honest, well-resourced verifier to:\n- Monitor all state assertions continuously\n- Detect invalid transitions through independent re-execution\n- Submit timely challenges with sufficient stake\n- Participate in dispute resolution through completion\n\nThis is weaker than the honest majority assumption required by L1 consensus but still represents a meaningful trust assumption. If all verifiers are compromised, collude, or are censored, invalid state transitions could be finalized.\n\n**Quantitative Challenge Period Analysis**\n\nThe 7-day period must be sufficient for:\n\n1. **Detection time**: Verifiers must notice invalid assertions. With continuous monitoring, this is near-instant; without it, detection depends on monitoring frequency.\n\n2. **Proof generation time**: Constructing fraud proofs requires re-executing the disputed computation and generating Merkle proofs. For complex state transitions, this may require hours.\n\n3. **L1 inclusion time**: The fraud proof transaction must be included in an L1 block. Under normal conditions, this takes minutes; under congestion, potentially hours.\n\n4. **Dispute resolution time**: Interactive protocols (Arbitrum) require multiple rounds of on-chain interaction, potentially taking days.\n\n**Adversarial Conditions Analysis**\n\nUnder adversarial conditions, the 7-day window may be stressed:\n\n*Gas Price Attack Scenario*:\n- Attacker submits invalid state root\n- Simultaneously floods L1 mempool to spike gas prices\n- If gas prices exceed verifier budgets, fraud proofs may be delayed\n\n*Quantitative Analysis*:\n```\nAssumptions:\n- Fraud proof gas cost: ~1-5M gas\n- Sustained attack gas price: 500 gwei\n- Fraud proof cost at 500 gwei: 0.5-2.5 ETH per proof\n- Verifier budget: Variable\n\nAttack sustainability:\n- Ethereum processes ~15M gas/block, ~7200 blocks/day\n- Sustaining 500 gwei for 7 days requires mass transaction spam\n- Estimated attack cost: >$100M in gas fees alone\n- Detection: Highly visible, would trigger social intervention\n```\n\nThis analysis suggests that pure gas price attacks are economically impractical for sustained periods, though short-term censorship remains possible.\n\n*L1 Censorship Scenario*:\n- If >50% of L1 proposers collude to censor fraud proofs\n- Censorship must be sustained for full 7 days\n- Highly detectable through missed slots and inclusion delays\n- Would likely trigger social consensus intervention (hard fork consideration)\n\n**Builder Centralization and PBS Dynamics**\n\nThe validator-count analysis above understates censorship risk due to Proposer-Builder Separation (PBS) dynamics. Currently:\n\n- ~3 builders produce >90% of Ethereum blocks through MEV-boost\n- Builders, not validators, determine transaction inclusion in most blocks\n- A colluding set of dominant builders could censor fraud proofs more easily than the ~900K validator count suggests\n\n*Revised Censorship Analysis*:\n```\nScenario: Top 3 builders collude to censor fraud proofs\n\nBlock production share: ~90%\nProbability of 7 days without inclusion through honest builder:\n- ~10% of blocks from non-colluding builders\n- ~7200 blocks/day \u00d7 7 days = 50,400 blocks\n- Expected honest blocks: ~5,040\n- Probability of censorship success: negligible\n\nHowever, for shorter windows or higher builder concentration:\n- If 99% builder collusion: ~504 honest blocks over 7 days\n- Still likely sufficient, but margin reduced significantly\n```\n\nThe PBS architecture creates a more concentrated censorship surface than pure validator analysis suggests. Mitigations include:\n- Inclusion lists (proposed protocol change forcing proposers to include certain transactions)\n- Builder diversity initiatives\n- Direct validator submission bypassing builders (higher cost, lower MEV extraction)\n\n**Minimum Verifier Capital Requirements**\n\nFor a verifier to guarantee fraud proof submission:\n\n```\nMinimum Capital = Stake Requirement + Max Gas Costs + Operational Buffer\n\nArbitrum:\n- Stake: 3,600 ETH (~$10M)\n- Gas (worst case 7-day congestion): ~10 ETH\n- Operational costs: Variable\n- Total: ~$10M minimum\n\nOptimism:\n- Bond: 0.08 ETH per challenge\n- Gas costs: Similar to Arbitrum\n- Lower barrier but also lower economic security\n```\n\n**Economic Sustainability Under Sustained Attack**\n\nThe forced inclusion escape hatch's economic sustainability under sustained attack conditions deserves analysis:\n\n```\nSustained Attack Scenario:\n- Attacker maintains L1 gas prices at 500 gwei for 7 days\n- User forced inclusion cost: ~100K gas \u00d7 500 gwei = 0.05 ETH per transaction\n- At $3000/ETH: ~$150 per forced transaction\n\nImplications:\n- Small-value transactions become uneconomical to force-include\n- Users with <$150 at risk may be unable to exit economically\n- Attacker cost to maintain: >$100M (as calculated above)\n- Rational attacker targets high-value situations where gains exceed attack cost\n```\n\n**Challenge Period Adequacy for Withdrawals**\n\nThe challenge period analysis must specifically address withdrawal security:\n\n1. **Withdrawal proof construction**: Users must generate Merkle proofs against the finalized state root. If the state root is fraudulent, honest verifiers must challenge before users can withdraw against the invalid state.\n\n2. **Withdrawal censorship**: An attacker could:\n   - Submit invalid state root\n   - Censor fraud proofs (as analyzed above)\n   - Censor withdrawal transactions from victims\n   - Execute withdrawals for attacker-controlled accounts\n\n3. **Withdrawal contract attack surface**: Even with valid state roots, withdrawal contract bugs could allow:\n   - Proof verification bypass\n   - Double withdrawals\n   - Unauthorized withdrawals\n\nThese withdrawal-specific concerns are addressed in detail in Section 5.\n\n**Challenge Period Adequacy Conclusion**\n\nThe 7-day period appears adequate under realistic adversarial models given:\n- Economic cost of sustained censorship/congestion attacks\n- High visibility of such attacks enabling social response\n- Multiple independent verifiers reducing single-point-of-failure risk\n\nHowever, the period assumes verifiers have sufficient capital and operational capability, and builder centralization creates a more concentrated censorship surface than validator counts suggest. Proposals for dynamic challenge periods (extending under detected adversarial conditions) could provide additional security margins.\n\n### 3.3 Sequencer Security\n\nThe sequencer is responsible for ordering transactions, executing them, and posting batches to L1. Sequencer security encompasses multiple dimensions:\n\n**Centralized Sequencer Risks**\n\nCurrent major optimistic rollups (Arbitrum One, OP Mainnet) operate single, permissioned sequencers controlled by their respective foundations. This introduces several risks:\n\n1. **Liveness failures**: Sequencer downtime halts L2 transaction processing\n2. **Censorship**: Sequencer can selectively exclude transactions\n3. **MEV extraction**: Sequencer has monopoly on transaction ordering\n4. **Front-running**: Sequencer can observe and front-run pending transactions\n\n**Forced Inclusion Mechanism Analysis**\n\nForced inclusion provides censorship resistance by allowing users to submit transactions directly to L1:\n\n*Arbitrum Delayed Inbox*:\n- Users submit transactions to L1 `SequencerInbox` contract\n- Sequencer must include within 24 hours or users can force inclusion\n- Force inclusion requires L1 transaction (~100K gas)\n\n*Security Analysis*:\n```\nScenario: Sequencer censors user, user attempts forced inclusion\n\nTimeline:\nT+0: User submits to delayed inbox\nT+24h: Force inclusion available if not included\nT+24h+: User calls forceInclusion()\n\nAdversarial Case: Sequencer AND L1 proposers collude\n- L1 proposers could censor forceInclusion() calls\n- Requires >50% L1 proposer collusion\n- Same security assumption as L1 censorship resistance\n```\n\nThe forced inclusion mechanism effectively inherits L1's censorship resistance properties with a 24-hour delay. Under the assumption that L1 remains censorship-resistant, users can always eventually transact.\n\n**24-Hour Delay Adequacy**\n\nThe 24-hour forced inclusion delay may be insufficient under extreme L1 congestion:\n\n```\nExtreme Congestion Scenario:\n- Base fee: 1000 gwei (historical peaks have exceeded this)\n- Force inclusion gas: 100K\n- Cost: 0.1 ETH (~$300)\n- Block capacity: ~1500 transactions per block\n- If demand exceeds capacity for >24 hours, forced inclusion may be delayed\n\nHistorical precedent:\n- NFT mints have caused multi-hour congestion\n- No 24-hour sustained congestion observed to date\n- But unprecedented events (major exploit, market crash) could trigger\n```\n\n**Sequencer-L1 Proposer Collusion**\n\nA sophisticated attack involves collusion between the sequencer and L1 block proposers:\n\n1. Sequencer submits invalid state root\n2. Colluding L1 proposers censor fraud proofs\n3. If sustained for 7 days, invalid state finalizes\n\n*Mitigation Analysis*:\n- Requires controlling >50% of L1 proposing power for 7 days\n- Current Ethereum has ~900K validators; controlling majority requires ~$15B+ stake\n- However, builder centralization (3 builders controlling 90% of blocks) reduces this barrier\n- Attack is detectable through inclusion delay metrics\n- Social layer would likely intervene before 7 days\n\nThis attack vector, while theoretically possible, requires resources and coordination that make it impractical against well-monitored rollups\u2014but builder centralization makes it less impractical than pure validator analysis suggests.\n\n**Decentralized Sequencer Roadmaps**\n\nBoth Arbitrum and Optimism have announced plans for sequencer decentralization:\n\n- **Arbitrum Timeboost**: Auction mechanism for transaction ordering rights within time windows, distributing MEV while maintaining performance\n- **Optimism**: Sequencer rotation within Superchain ecosystem, with plans for permissionless sequencing\n\nThese approaches aim to distribute MEV and reduce single points of failure while maintaining performance characteristics.\n\n---\n\n## 4. Zero-Knowledge Rollup Security Mechanisms\n\n### 4.1 Validity Proof Systems\n\nZK-rollups replace the challenge period with cryptographic validity proofs, fundamentally altering the security model.\n\n**Proof System Taxonomy**\n\n| System | Proof Size | Verification Cost | Prover Complexity | Trust Setup |\n|--------|-----------|-------------------|-------------------|-------------|\n| Groth16 | ~200 bytes | ~200K gas (~3 pairings) | O(n log n) FFTs + O(n) MSMs | Trusted (per-circuit) |\n| PLONK | ~400 bytes | ~300K gas (~2 pairings + O(1) field ops) | O(n log n) FFTs + O(n) MSMs | Universal (updatable) |\n| STARKs | ~50-200 KB | ~1-2M gas (hash-based) | O(n log\u00b2 n) hash operations | Transparent |\n| Halo2 | ~5-10 KB | ~500K gas | O(n log n) + recursion overhead | Transparent |\n\n**Formal Security Definitions**\n\nValidity proofs must satisfy precise security properties:\n\n1. **Completeness**: For any valid statement x with witness w, an honest prover can produce a proof \u03c0 that verifies.\n   ```\n   Pr[Verify(vk, x, Prove(pk, x, w)) = 1] = 1\n   ```\n\n2. **Computational Soundness**: No probabilistic polynomial-time (PPT) adversary can produce a valid proof for a false statement except with negligible probability.\n   ```\n   Pr[Verify(vk, x, \u03c0) = 1 \u2227 x \u2209 L] \u2264 negl(\u03bb)\n   ```\n\n3. **Knowledge Soundness**: For ZK-rollups, we require the stronger property that any prover producing a valid proof must \"know\" a valid witness. Formally, there exists an extractor E such that:\n   ```\n   Pr[Verify(vk, x, \u03c0) = 1 \u2227 (x, w) \u2209 R] \u2264 negl(\u03bb)\n   ```\n   where w = E(prover's internal state).\n\nKnowledge soundness is critical for rollups because it ensures that a valid proof corresponds to an actual valid state transition, not merely that invalid proofs are hard to forge.\n\n4. **Zero-Knowledge**: The proof reveals nothing about the witness beyond the statement's truth. For rollups, this property is often relaxed since transaction data is typically public anyway.\n\n**Soundness Error and Security Parameters**\n\nThe soundness error\u2014probability of a false proof verifying\u2014depends on the proof system and parameters:\n\n- **Groth16**: Soundness error \u2248 1/|F| where F is the field. For BN254 (common choice), |F| \u2248 2^254, giving ~254-bit soundness. The reduction to q-SDH is tight, meaning the concrete security matches the assumption's hardness.\n\n- **PLONK**: Field-based soundness with additional considerations:\n  - Polynomial commitment scheme (typically KZG) adds security dependency on discrete log in pairing groups\n  - Fiat-Shamir transformation introduces security loss in the random oracle model\n  - Algebraic Group Model (AGM) proofs provide security reductions but AGM is a strong idealization\n  - Concrete security: ~128-bit after accounting for reduction tightness and Fiat-Shamir\n\n- **STARKs**: Soundness depends on",
  "manuscript_v1": "# Rollup Security Mechanisms: A Comprehensive Analysis of Trust Assumptions, Verification Paradigms, and Emerging Threat Vectors\n\n## Executive Summary\n\nRollups have emerged as the dominant scaling paradigm for blockchain networks, processing over $50 billion in total value locked (TVL) across major implementations as of late 2024. These Layer 2 (L2) solutions execute transactions off-chain while inheriting security guarantees from underlying Layer 1 (L1) networks through cryptographic and economic mechanisms. This report provides a comprehensive examination of rollup security architectures, analyzing the fundamental trust assumptions, verification mechanisms, and attack surfaces that define the security posture of these systems.\n\nOur analysis reveals that rollup security is not monolithic but rather a composite of multiple interdependent mechanisms including state commitment schemes, fraud and validity proofs, data availability guarantees, sequencer designs, and bridge architectures. We examine the two primary rollup paradigms\u2014optimistic rollups and zero-knowledge (ZK) rollups\u2014identifying their respective security tradeoffs, maturity levels, and vulnerability profiles.\n\nKey findings indicate that while rollups significantly improve scalability, they introduce novel trust assumptions often overlooked in simplified security models. Current implementations frequently rely on training wheels\u2014centralized components such as upgradeable contracts, permissioned sequencers, and security councils\u2014that deviate from the trustless ideal. We identify critical attack vectors including sequencer manipulation, data withholding attacks, bridge exploits, and proof system vulnerabilities, providing quantitative analysis of historical incidents and their root causes.\n\nThe report concludes with an assessment of emerging security trends, including shared sequencer networks, based rollups, proof aggregation, and formal verification advances. We argue that rollup security will increasingly depend on defense-in-depth strategies combining cryptographic guarantees with economic incentives and social consensus mechanisms.\n\n---\n\n## 1. Introduction\n\n### 1.1 The Scaling Imperative and Rollup Emergence\n\nBlockchain scalability has remained a persistent challenge since Bitcoin's inception. Ethereum's mainnet processes approximately 15-30 transactions per second (TPS), fundamentally constraining its utility for global-scale applications. Various scaling approaches have been proposed, including sharding, state channels, plasma, and sidechains, each presenting distinct security tradeoffs.\n\nRollups emerged from this landscape as a particularly compelling solution, first conceptualized in detail by Barry Whitehat in 2018 and subsequently formalized through implementations like Optimism, Arbitrum, zkSync, and StarkNet. The rollup paradigm's core insight is the separation of execution from consensus: transactions are executed off-chain by specialized operators while transaction data and state commitments are posted to the L1, enabling independent verification.\n\nThis architecture achieves scalability through compression and batching\u2014a single L1 transaction can represent thousands of L2 transactions\u2014while theoretically preserving L1 security guarantees. However, the precise nature of these security guarantees and the conditions under which they hold require careful examination.\n\n### 1.2 Scope and Methodology\n\nThis report examines rollup security mechanisms through multiple analytical lenses:\n\n1. **Cryptographic security**: Proof systems, commitment schemes, and cryptographic assumptions\n2. **Economic security**: Incentive structures, stake requirements, and game-theoretic properties\n3. **Operational security**: Sequencer designs, upgrade mechanisms, and governance structures\n4. **Systemic security**: Cross-layer interactions, composability risks, and failure modes\n\nOur analysis draws on protocol specifications, academic literature, audit reports, and empirical data from production deployments. We adopt a threat modeling approach, systematically identifying adversarial capabilities and evaluating countermeasures.\n\n---\n\n## 2. Foundational Security Architecture\n\n### 2.1 State Commitment and Verification\n\nThe fundamental security primitive in rollups is the state commitment\u2014a cryptographic representation of the L2 state posted to L1. This commitment enables verification that L2 state transitions follow protocol rules without requiring L1 nodes to re-execute all L2 transactions.\n\n**Merkle Tree Commitments**\n\nMost rollups employ Merkle tree structures to commit to state. The state root, a 32-byte hash, represents the entire L2 state including account balances, contract storage, and nonces. Arbitrum and Optimism use variants of Ethereum's Modified Merkle Patricia Trie, while zkSync Era employs a sparse Merkle tree optimized for ZK circuit efficiency.\n\nThe security of Merkle commitments relies on collision resistance of the underlying hash function. For SHA-256 or Keccak-256, finding collisions requires approximately 2^128 operations, providing 128-bit security. However, ZK-rollups often use algebraically structured hash functions like Poseidon or Rescue, which enable efficient circuit representation but have received less cryptanalytic scrutiny.\n\n**State Transition Verification**\n\nThe verification mechanism differentiates rollup paradigms:\n\n- **Optimistic rollups** assume state transitions are valid unless challenged. A challenge period (typically 7 days) allows observers to submit fraud proofs demonstrating invalid transitions.\n\n- **ZK-rollups** require validity proofs\u2014cryptographic proofs that state transitions are correct\u2014before L1 acceptance. No challenge period is necessary as validity is mathematically guaranteed.\n\n### 2.2 Data Availability Requirements\n\nData availability (DA) is arguably the most critical security property for rollups. Users must be able to reconstruct the L2 state from publicly available data to verify correctness and exit to L1 if necessary.\n\n**The Data Availability Problem**\n\nConsider an adversarial sequencer that posts a valid state root but withholds the underlying transaction data. Without this data:\n\n- Users cannot verify the state transition's correctness\n- Users cannot construct fraud proofs (optimistic rollups)\n- Users cannot generate inclusion proofs for withdrawals\n- The rollup state becomes effectively frozen\n\n**DA Solutions Spectrum**\n\nCurrent implementations employ various DA strategies:\n\n| DA Layer | Examples | Security Model | Cost (per byte) |\n|----------|----------|----------------|-----------------|\n| Ethereum calldata | Arbitrum One, Optimism | Ethereum consensus | ~16 gas |\n| Ethereum blobs (EIP-4844) | Most major rollups | Ethereum consensus | ~1 gas equivalent |\n| Dedicated DA layers | Celestia, EigenDA, Avail | Independent consensus | Variable |\n| Validiums | Immutable X, some StarkEx | Committee/DAC | Minimal |\n\nThe security implications are significant. Ethereum-native DA inherits full Ethereum security, requiring an attacker to control Ethereum consensus to withhold data. External DA layers introduce additional trust assumptions\u2014users must trust the DA layer's consensus mechanism and honest majority assumptions.\n\n**EIP-4844 and Danksharding**\n\nEthereum's Dencun upgrade (March 2024) introduced blob transactions, providing dedicated DA space for rollups. Blobs offer approximately 10x cost reduction compared to calldata while maintaining Ethereum's security guarantees. Each blob contains 128 KB of data, with a target of 3 blobs per block (384 KB) and maximum of 6 blobs (768 KB).\n\nThe security model for blobs differs subtly from calldata. Blob data is guaranteed available for approximately 18 days (4096 epochs) but is not permanently stored by consensus nodes. Rollups must ensure archival solutions exist for historical data reconstruction.\n\n---\n\n## 3. Optimistic Rollup Security Mechanisms\n\n### 3.1 Fraud Proof Systems\n\nOptimistic rollups derive their name from the optimistic assumption that posted state roots are valid. Security relies on the ability to prove and penalize invalid state transitions through fraud proofs.\n\n**Interactive Fraud Proofs (Arbitrum)**\n\nArbitrum employs a multi-round interactive dispute resolution protocol:\n\n1. **Assertion**: Validator posts state commitment with stake (currently ~$1M equivalent)\n2. **Challenge**: Observer disputes by staking and identifying disagreement point\n3. **Bisection**: Parties iteratively narrow disagreement to single instruction\n4. **One-step proof**: L1 contract executes single WAVM instruction to determine correctness\n5. **Resolution**: Losing party's stake is slashed\n\nThis design minimizes on-chain computation\u2014only one instruction is ever executed on L1\u2014while maintaining security. The protocol is secure under the assumption that at least one honest validator monitors the chain and can submit challenges within the dispute window.\n\n```\nDispute Resolution Complexity:\n- Rounds required: O(log n) where n = number of instructions in disputed block\n- On-chain verification: O(1) - single instruction execution\n- Total time: ~7 days (challenge period) + bisection rounds\n```\n\n**Non-Interactive Fraud Proofs (Optimism)**\n\nOptimism's Cannon fault proof system (launched 2024) generates non-interactive proofs:\n\n1. **Assertion**: Proposer posts output root with bond\n2. **Challenge**: Challenger posts competing claim with bond\n3. **Bisection game**: On-chain bisection to identify first divergent state\n4. **Execution**: MIPS VM executes disputed instruction on-chain\n\nThe key innovation is the use of a minimal MIPS instruction set, enabling deterministic re-execution of any disputed computation. This approach reduces trust assumptions compared to earlier implementations that relied on a Security Council for dispute resolution.\n\n### 3.2 Challenge Period Security Analysis\n\nThe 7-day challenge period is a critical security parameter with significant implications:\n\n**Honest Verifier Assumption**\n\nSecurity requires at least one honest, well-resourced verifier to:\n- Monitor all state assertions\n- Detect invalid transitions\n- Submit timely challenges\n- Participate in dispute resolution\n\nThis is weaker than the honest majority assumption required by L1 consensus but still represents a meaningful trust assumption. If all verifiers are compromised, collude, or are censored, invalid state transitions could be finalized.\n\n**Censorship Resistance**\n\nChallengers must be able to submit fraud proofs to L1 within the challenge window. An adversary controlling L1 block production could theoretically censor fraud proofs, though this would require sustained censorship for 7 days\u2014an attack detectable and addressable through social consensus.\n\n**Economic Security**\n\nThe stake required for assertions provides economic security:\n\n```\nMinimum Attack Cost = Stake \u00d7 (1 + Expected Challenges)\nCurrent Arbitrum Stake \u2248 3,600 ETH \u2248 $10M (at $2,800/ETH)\n```\n\nHowever, this economic security is bounded. A sufficiently motivated attacker with resources exceeding the stake could potentially profit from invalid state transitions affecting high-value positions.\n\n### 3.3 Sequencer Security\n\nThe sequencer is responsible for ordering transactions, executing them, and posting batches to L1. Sequencer security encompasses multiple dimensions:\n\n**Centralized Sequencer Risks**\n\nCurrent major optimistic rollups (Arbitrum One, OP Mainnet) operate single, permissioned sequencers controlled by their respective foundations. This introduces several risks:\n\n1. **Liveness failures**: Sequencer downtime halts L2 transaction processing\n2. **Censorship**: Sequencer can selectively exclude transactions\n3. **MEV extraction**: Sequencer has monopoly on transaction ordering\n4. **Front-running**: Sequencer can observe and front-run pending transactions\n\n**Mitigation Mechanisms**\n\nRollups implement various mitigations:\n\n- **Forced inclusion**: Users can submit transactions directly to L1 that must be included within a timeout (typically 24 hours in Arbitrum)\n- **Escape hatches**: Users can withdraw to L1 without sequencer cooperation\n- **Sequencer bonds**: Economic penalties for misbehavior (limited implementation)\n\n**Decentralized Sequencer Roadmaps**\n\nBoth Arbitrum and Optimism have announced plans for sequencer decentralization:\n\n- **Arbitrum**: Timeboost auction mechanism for transaction ordering rights\n- **Optimism**: Sequencer rotation within Superchain ecosystem\n\nThese approaches aim to distribute MEV and reduce single points of failure while maintaining performance characteristics.\n\n---\n\n## 4. Zero-Knowledge Rollup Security Mechanisms\n\n### 4.1 Validity Proof Systems\n\nZK-rollups replace the challenge period with cryptographic validity proofs, fundamentally altering the security model.\n\n**Proof System Taxonomy**\n\n| System | Proof Size | Verification Cost | Prover Time | Trust Setup |\n|--------|-----------|-------------------|-------------|-------------|\n| Groth16 | ~200 bytes | ~200K gas | Minutes-hours | Trusted (per-circuit) |\n| PLONK | ~400 bytes | ~300K gas | Minutes-hours | Universal |\n| STARKs | ~50-200 KB | ~1-2M gas | Seconds-minutes | Transparent |\n| Halo2 | ~5-10 KB | ~500K gas | Minutes | Transparent |\n\n**Security Properties**\n\nValidity proofs provide two key properties:\n\n1. **Soundness**: Invalid state transitions cannot produce valid proofs (except with negligible probability)\n2. **Completeness**: Valid state transitions always produce valid proofs\n\nThe soundness guarantee is computational\u2014an adversary with bounded computational resources cannot forge proofs. Security parameters are typically set to provide 128-bit security, meaning proof forgery requires approximately 2^128 operations.\n\n**Cryptographic Assumptions**\n\nDifferent proof systems rely on different hardness assumptions:\n\n- **Groth16/PLONK**: Discrete logarithm problem, knowledge of exponent assumption\n- **STARKs**: Collision-resistant hash functions only (quantum-resistant)\n- **Halo2**: Discrete logarithm in elliptic curve groups\n\nThe reliance on elliptic curve cryptography in SNARKs introduces potential quantum vulnerability. STARKs' hash-based construction provides post-quantum security but at the cost of larger proof sizes.\n\n### 4.2 Circuit Security and Formal Verification\n\nZK-rollups require encoding the state transition function as an arithmetic circuit. Circuit bugs represent a critical attack vector.\n\n**Historical Vulnerabilities**\n\nSeveral significant circuit vulnerabilities have been discovered:\n\n1. **zkSync Era (2023)**: Vulnerability in ECRECOVER precompile implementation could allow signature forgery (patched before exploitation)\n2. **Polygon zkEVM (2024)**: Soundness bug in Keccak circuit could allow proof of invalid state transitions (found by security researchers)\n3. **Various**: Multiple issues found through audit processes and bug bounties\n\n**Formal Verification Approaches**\n\nLeading ZK-rollups invest heavily in formal verification:\n\n- **StarkNet**: Cairo language designed for formal verification compatibility\n- **zkSync**: Extensive use of formal methods for circuit verification\n- **Polygon**: Collaboration with academic institutions on formal proofs\n\n```rust\n// Example: Formal specification of balance transfer constraint\n// Must prove: sender_balance_before >= amount\n// Must prove: sender_balance_after = sender_balance_before - amount\n// Must prove: receiver_balance_after = receiver_balance_before + amount\nconstraint balance_conservation {\n    sender_before - amount == sender_after\n    receiver_before + amount == receiver_after\n    sender_before >= amount  // Prevent underflow\n}\n```\n\nDespite these efforts, the complexity of EVM-equivalent circuits (millions of constraints) makes complete formal verification challenging. Defense in depth through audits, bug bounties, and gradual rollout remains essential.\n\n### 4.3 Prover Infrastructure Security\n\nThe prover\u2014the entity generating validity proofs\u2014represents another security consideration.\n\n**Centralized Provers**\n\nCurrent ZK-rollups operate centralized proving infrastructure:\n\n- **StarkNet**: StarkWare operates provers\n- **zkSync Era**: Matter Labs operates provers\n- **Polygon zkEVM**: Polygon Labs operates provers\n\nCentralized provers create liveness dependencies but not safety risks\u2014an adversary controlling the prover cannot generate proofs for invalid state transitions (assuming sound proof system).\n\n**Prover Decentralization Challenges**\n\nDecentralizing proving is technically challenging due to:\n\n1. **Hardware requirements**: Proving requires significant computational resources (hundreds of GB RAM, powerful CPUs/GPUs)\n2. **Proof generation time**: Must meet block time requirements\n3. **Economic sustainability**: Proving costs must be covered by fees\n\nEmerging solutions include proof markets (e.g., =nil; Foundation's Proof Market) and hardware acceleration through ASICs and FPGAs.\n\n---\n\n## 5. Bridge Security\n\n### 5.1 Canonical Bridge Architecture\n\nBridges enabling asset transfer between L1 and L2 represent critical security infrastructure. The canonical bridge is the official mechanism for deposits and withdrawals.\n\n**Deposit Flow**\n\n```\nUser \u2192 L1 Bridge Contract \u2192 Event Emission \u2192 Sequencer Detection \u2192 L2 Minting\n```\n\nDeposits are typically fast (minutes) as they require only L1 confirmation. The L2 mints equivalent assets upon detecting the L1 deposit event.\n\n**Withdrawal Flow (Optimistic)**\n\n```\nUser \u2192 L2 Burn \u2192 State Root Inclusion \u2192 Challenge Period (7 days) \u2192 L1 Claim\n```\n\nWithdrawals require waiting for the challenge period, creating significant UX friction but ensuring security.\n\n**Withdrawal Flow (ZK)**\n\n```\nUser \u2192 L2 Burn \u2192 Validity Proof Generation \u2192 L1 Verification \u2192 Immediate Claim\n```\n\nZK-rollups enable faster withdrawals (hours rather than days) as validity proofs provide immediate finality.\n\n### 5.2 Bridge Attack Vectors\n\nBridges have been the primary target for rollup-related exploits:\n\n**Historical Bridge Exploits (L2-Related)**\n\n| Incident | Date | Loss | Root Cause |\n|----------|------|------|------------|\n| Ronin Bridge | Mar 2022 | $625M | Compromised validator keys |\n| Wormhole | Feb 2022 | $320M | Signature verification bug |\n| Nomad | Aug 2022 | $190M | Merkle proof validation flaw |\n\nWhile these examples include non-rollup bridges, they illustrate common vulnerability patterns:\n\n1. **Key management failures**: Multisig compromise, inadequate key security\n2. **Smart contract bugs**: Verification logic errors, reentrancy\n3. **Cryptographic implementation flaws**: Incorrect signature schemes, proof validation\n\n**Rollup-Specific Bridge Risks**\n\nCanonical rollup bridges face additional risks:\n\n- **Upgrade mechanism exploitation**: Malicious contract upgrades draining bridge funds\n- **Sequencer collusion**: Sequencer cooperation with bridge exploit\n- **Data availability attacks**: Preventing users from proving withdrawal eligibility\n\n### 5.3 Bridge Security Mechanisms\n\n**Time-Locked Upgrades**\n\nMost rollup bridges implement time-locked upgrade mechanisms:\n\n```\nArbitrum: 12-day timelock for non-emergency upgrades\nOptimism: 7-day timelock with Guardian override capability\nzkSync: Security Council with 21-day standard timelock\n```\n\nThese timelocks allow users to exit before potentially malicious upgrades take effect, though they assume users actively monitor governance proposals.\n\n**Security Councils**\n\nSecurity councils\u2014multisig groups with emergency powers\u2014provide a backstop against critical vulnerabilities:\n\n- **Arbitrum Security Council**: 12-member multisig (9/12 threshold for emergency actions)\n- **Optimism Security Council**: Similar structure with defined emergency procedures\n\nSecurity councils represent a trust assumption\u2014users must trust council members not to collude maliciously. This is often described as \"training wheels\" to be removed as systems mature.\n\n---\n\n## 6. Emerging Security Paradigms\n\n### 6.1 Shared Sequencing\n\nShared sequencing networks aim to provide decentralized, credibly neutral transaction ordering across multiple rollups.\n\n**Espresso Systems**\n\nEspresso's HotShot consensus provides:\n- BFT consensus among sequencer nodes\n- Atomic cross-rollup transactions\n- MEV redistribution mechanisms\n\nSecurity model: Assumes honest majority among sequencer nodes, with economic penalties for misbehavior.\n\n**Astria**\n\nAstria's shared sequencer offers:\n- Lazy sequencing (ordering without execution)\n- Rollup-agnostic design\n- Decentralized sequencer set\n\n**Security Implications**\n\nShared sequencing introduces new trust assumptions:\n- Users must trust the shared sequencer network's consensus\n- Cross-rollup atomicity creates systemic risk\n- Sequencer network liveness affects multiple rollups simultaneously\n\n### 6.2 Based Rollups\n\nBased rollups (proposed by Justin Drake) delegate sequencing to L1 proposers, inheriting L1's decentralization and censorship resistance.\n\n**Security Properties**\n\n- **Liveness**: Equivalent to L1 (no separate sequencer to fail)\n- **Censorship resistance**: Equivalent to L1\n- **MEV**: Flows to L1 validators rather than centralized sequencer\n\n**Tradeoffs**\n\n- Slower confirmation times (L1 block times)\n- Reduced MEV capture for rollup ecosystem\n- Increased complexity for L1 proposers\n\n### 6.3 Proof Aggregation\n\nProof aggregation combines multiple validity proofs into a single proof, reducing verification costs and enabling new security architectures.\n\n**SHARP (StarkWare)**\n\nStarkWare's Shared Prover (SHARP) aggregates proofs from multiple applications:\n- Amortizes verification costs across applications\n- Enables smaller applications to afford ZK security\n- Creates dependencies between applications\n\n**Polygon AggLayer**\n\nPolygon's aggregation layer aims to:\n- Unify liquidity across Polygon chains\n- Provide unified security guarantees\n- Enable cross-chain atomicity\n\n**Security Considerations**\n\nProof aggregation introduces systemic risk\u2014a bug in the aggregation layer affects all dependent rollups. However, it also concentrates security investment, potentially improving overall security through shared auditing and formal verification efforts.\n\n### 6.4 Multi-Prover Architectures\n\nMulti-prover systems require agreement from multiple independent proof systems before accepting state transitions.\n\n**Taiko's Approach**\n\nTaiko implements a multi-prover model:\n- SGX-based proofs for fast, approximate verification\n- ZK proofs for cryptographic guarantees\n- Economic security through staking\n\n**Security Analysis**\n\nMulti-prover architectures provide defense in depth:\n```\nP(successful attack) = P(break prover 1) \u00d7 P(break prover 2) \u00d7 ... \u00d7 P(break prover n)\n```\n\nWith independent proof systems, security compounds multiplicatively. However, this assumes true independence\u2014shared cryptographic assumptions or implementation dependencies reduce actual security gains.\n\n---\n\n## 7. Quantitative Security Analysis\n\n### 7.1 Economic Security Metrics\n\n**Total Value Secured**\n\nAs of late 2024:\n- Arbitrum One: ~$15B TVL\n- OP Mainnet: ~$7B TVL\n- Base: ~$8B TVL\n- zkSync Era: ~$1B TVL\n- StarkNet: ~$500M TVL\n\n**Security Budget Analysis**\n\n```\nArbitrum Economic Security:\n- Validator stake: ~$10M\n- Security Council: 12 members, ~$100M+ combined reputation\n- Bug bounty: Up to $2M per vulnerability\n\nRatio Analysis:\n- Stake/TVL: 0.07% (relatively low)\n- Implicit security from reputation and legal liability: Difficult to quantify\n```\n\nThis analysis reveals that explicit economic security (stake) is relatively low compared to TVL. Security relies heavily on implicit factors including reputation, legal liability, and the technical difficulty of attacks.\n\n### 7.2 Attack Cost Analysis\n\n**Optimistic Rollup Attack Scenarios**\n\n| Attack | Requirements | Estimated Cost | Detection Probability |\n|--------|--------------|----------------|----------------------|\n| Invalid state root | Compromise all verifiers | Variable | High (public data) |\n| Censorship (7 days) | Control L1 block production | >$1B (51% attack) | Very high |\n| Sequencer manipulation | Compromise sequencer | Operational security dependent | Medium |\n\n**ZK-Rollup Attack Scenarios**\n\n| Attack | Requirements | Estimated Cost | Detection Probability |\n|--------|--------------|----------------|----------------------|\n| Proof forgery | Break cryptographic assumptions | Computationally infeasible | N/A |\n| Circuit bug exploitation | Discover and exploit 0-day | Variable (bug bounty arbitrage) | Low initially |\n| Prover denial of service | Overwhelm proving infrastructure | Moderate | High |\n\n### 7.3 Incident Analysis\n\n**Significant Security Incidents (2022-2024)**\n\n1. **Optimism (2022)**: $20M bug bounty paid for critical vulnerability in Geth fork that could have allowed infinite ETH minting\n\n2. **Arbitrum (2022)**: $400K bug bounty for vulnerability allowing theft of deposits in progress\n\n3. **zkSync (2023)**: Multiple critical vulnerabilities found in audits and bug bounty program before mainnet exploitation\n\nThese incidents demonstrate that:\n- Significant vulnerabilities exist even in well-audited systems\n- Bug bounty programs provide crucial security layer\n- Gradual rollout with limited TVL reduces impact of undiscovered vulnerabilities\n\n---\n\n## 8. Practical Security Recommendations\n\n### 8.1 For Users\n\n1. **Understand withdrawal delays**: Optimistic rollup withdrawals require 7+ days; plan accordingly\n2. **Monitor governance**: Track upgrade proposals that could affect fund security\n3. **Diversify across rollups**: Avoid concentration risk from single-rollup vulnerabilities\n4. **Use canonical bridges**: Third-party bridges introduce additional trust assumptions\n5. **Verify contract addresses**: Confirm interaction with official rollup contracts\n\n### 8.2 For Developers\n\n1. **Implement proper L1-L2 messaging**: Account for message delays and potential failures\n2. **Handle reorgs appropriately**: L2 reorgs are possible before L1 finalization\n3. **Test against rollup-specific behaviors**: Gas metering, precompiles, and opcodes may differ\n4. **Plan for upgrade scenarios**: Design contracts to handle rollup upgrades gracefully\n5. **Consider cross-rollup risks**: Composability across rollups introduces additional attack surfaces\n\n### 8.3 For Protocol Designers\n\n1. **Minimize trust assumptions**: Document and justify each trust assumption\n2. **Implement defense in depth**: Combine cryptographic, economic, and social security\n3. **Plan decentralization roadmap**: Credibly commit to removing training wheels\n4. **Invest in formal verification**: Prioritize mathematical proofs over informal arguments\n5. **Establish robust incident response**: Prepare for security incidents with clear procedures\n\n---\n\n## 9. Future Outlook\n\n### 9.1 Technology Trends\n\n**Proof System Evolution**\n\n- Continued improvement in prover efficiency (10-100x improvements expected)\n- Hybrid proof systems combining SNARK efficiency with STARK transparency\n- Hardware acceleration through dedicated proving chips\n\n**Interoperability**\n\n- Standardization of cross-rollup messaging protocols\n- Shared security through proof aggregation\n- Atomic cross-rollup transactions\n\n**Decentralization**\n\n- Progressive removal of training wheels\n- Decentralized sequencer networks\n- Community governance of upgrade processes\n\n### 9.2 Regulatory Considerations\n\nRegulatory clarity (or lack thereof) will impact rollup security:\n- KYC/AML requirements may affect sequencer decentralization\n- Securities classification could impact token-based security models\n- Data localization requirements may constrain DA layer choices\n\n### 9.3 Research Frontiers\n\nActive research areas with security implications:\n\n1. **Formal verification of ZK circuits**: Automated tools for proving circuit correctness\n2. **MEV mitigation**: Encrypted mempools, fair ordering protocols\n3. **Post-quantum cryptography**: Migration paths for SNARK-based rollups\n4. **Economic security formalization**: Rigorous frameworks for analyzing incentive compatibility\n\n---\n\n## 10. Conclusion\n\nRollup security mechanisms represent a sophisticated synthesis of cryptographic, economic, and social security primitives. This analysis reveals several key insights:\n\n**Security is Multidimensional**\n\nRollup security cannot be reduced to a single metric or mechanism. Comprehensive security requires:\n- Sound cryptographic foundations (proofs, commitments)\n- Robust data availability guarantees\n- Aligned economic incentives\n- Operational security practices\n- Governance mechanisms for adaptation\n\n**Trust Assumptions Persist**\n\nDespite aspirations toward trustlessness, current rollup implementations retain significant trust assumptions:\n- Security councils with emergency powers\n- Centralized sequencers\n- Upgradeable contracts\n- Permissioned validator sets\n\nThese training wheels are necessary during the maturation phase but represent meaningful deviations from the trustless ideal. Users and developers should understand and account for these assumptions.\n\n**Security is Dynamic**\n\nRollup security is not static\u2014it evolves through:\n- Discovery and patching of vulnerabilities\n- Maturation of proof systems and implementations\n- Decentralization of operational components\n- Development of formal verification tools\n\n**Defense in Depth is Essential**\n\nNo single security mechanism is sufficient. The most robust rollup architectures combine:\n- Multiple independent verification mechanisms\n- Economic penalties for misbehavior\n- Social consensus as ultimate backstop\n- Gradual rollout limiting exposure\n\nAs rollups continue to mature and secure increasing value, the importance of rigorous security analysis only grows. This report provides a foundation for understanding current security mechanisms while highlighting areas requiring continued research and development.\n\nThe rollup ecosystem stands at a critical juncture\u2014the technology has proven viable, but the path to fully trustless, decentralized operation remains incomplete. Success will require sustained investment in security research, formal verification, and careful operational practices. The stakes are high: rollups may ultimately secure trillions of dollars in value, making their security mechanisms among the most consequential technical systems of our time.\n\n---\n\n## References\n\n1. Buterin, V. (2021). \"An Incomplete Guide to Rollups.\" Ethereum Foundation Blog.\n\n2. Kalodner, H., et al. (2018). \"Arbitrum: Scalable, private smart contracts.\" USENIX Security Symposium.\n\n3. StarkWare. (2023). \"STARK Math Series.\" StarkWare Documentation.\n\n4. Optimism. (2024). \"Cannon Fault Proof System Specification.\" Optimism Documentation.\n\n5. Matter Labs. (2023). \"zkSync Era Security Documentation.\" Matter Labs.\n\n6. L2Beat. (2024). \"Layer 2 Risk Analysis Framework.\" l2beat.com.\n\n7. Drake, J. (2023). \"Based Rollups\u2014Superpowers from L1 Sequencing.\" Ethereum Research.\n\n8. Espresso Systems. (2023). \"HotShot Consensus Protocol Specification.\"\n\n9. Polygon. (2024). \"AggLayer Technical Documentation.\"\n\n10. Trail of Bits. (2023). \"Security Assessment of zkSync Era.\" Audit Report.\n\n---\n\n*Report prepared for academic and research purposes. Data current as of late 2024. Specific figures and implementations may have changed since publication.*"
}